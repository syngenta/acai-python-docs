{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Acai AWS \u00b6 DRY, configurable, declarative node library for working with Amazon Web Service Lambdas. Features \u00b6 Highly configurable apigateway internal router Openapi schema adherence for all event types Extensible and customizable middleware for validation and other tasks DRY coding interfaces without the need of boilerplate Ease-of-use with the serverless framework Local Development support Happy Path Programming (See Philosophy below) Philosophy \u00b6 The Acai philosophy is to provide a dry, configurable, declarative library for use with the amazon lambdas, which encourages Happy Path Programming (HPP). Happy Path Programming is an idea in which inputs are all validated before operated on. This ensures code follows the happy path without the need for mid-level, nested exceptions and all the nasty exception handling that comes with that. The library uses layers of customizable middleware options to allow a developer to easily dictate what constitutes a valid input, without nested conditionals, try/catch blocks or other coding blocks which distract from the happy path that covers the majority of that codes intended operation.","title":"Home"},{"location":"#acai-aws","text":"DRY, configurable, declarative node library for working with Amazon Web Service Lambdas.","title":"Acai AWS"},{"location":"#features","text":"Highly configurable apigateway internal router Openapi schema adherence for all event types Extensible and customizable middleware for validation and other tasks DRY coding interfaces without the need of boilerplate Ease-of-use with the serverless framework Local Development support Happy Path Programming (See Philosophy below)","title":"Features"},{"location":"#philosophy","text":"The Acai philosophy is to provide a dry, configurable, declarative library for use with the amazon lambdas, which encourages Happy Path Programming (HPP). Happy Path Programming is an idea in which inputs are all validated before operated on. This ensures code follows the happy path without the need for mid-level, nested exceptions and all the nasty exception handling that comes with that. The library uses layers of customizable middleware options to allow a developer to easily dictate what constitutes a valid input, without nested conditionals, try/catch blocks or other coding blocks which distract from the happy path that covers the majority of that codes intended operation.","title":"Philosophy"},{"location":"changes/","text":"Changes to the Acai AWS from 1.x to 2.0 \u00b6 In version 2.0 we have added a lot of cool new features, but that does require deprecating some old things. Below is a list of all the changes: Tip If you don't want to make the changes yourself manually, we have a script which will make the changes for you. Just run this command in your terminal from the root of the directory of the project you want to upgrade: 1 /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/syngenta/acai-python-docs/main/scripts/python-upgrade.sh ) \" APIGateway \u00b6 old new description router.route() router.route(event) router.route now requires the event to be passed in required_params required_query required_query is how you define required query string params request.params request.query_params request.query_params is how you access query string params DynamoDB Record \u00b6 old new record.event_id record.id record.event_name record.name record.event_source record.source record.event_source_arn record.source_arn S3 Record \u00b6 old new record.event_id record.id record.event_name record.name record.event_source record.source record.event_source_arn record.source_arn record.requestParameters record.request record.responseElements record.response record.s3SchemaVersion record.version SNS/SQS Record \u00b6 old new record.event_name record.name record.event_source record.source record.event_source_arn record.source_arn","title":"2.0 Breaking Changes"},{"location":"changes/#changes-to-the-acai-aws-from-1x-to-20","text":"In version 2.0 we have added a lot of cool new features, but that does require deprecating some old things. Below is a list of all the changes: Tip If you don't want to make the changes yourself manually, we have a script which will make the changes for you. Just run this command in your terminal from the root of the directory of the project you want to upgrade: 1 /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/syngenta/acai-python-docs/main/scripts/python-upgrade.sh ) \"","title":"Changes to the Acai AWS from 1.x to 2.0"},{"location":"changes/#apigateway","text":"old new description router.route() router.route(event) router.route now requires the event to be passed in required_params required_query required_query is how you define required query string params request.params request.query_params request.query_params is how you access query string params","title":"APIGateway"},{"location":"changes/#dynamodb-record","text":"old new record.event_id record.id record.event_name record.name record.event_source record.source record.event_source_arn record.source_arn","title":"DynamoDB Record"},{"location":"changes/#s3-record","text":"old new record.event_id record.id record.event_name record.name record.event_source record.source record.event_source_arn record.source_arn record.requestParameters record.request record.responseElements record.response record.s3SchemaVersion record.version","title":"S3 Record"},{"location":"changes/#snssqs-record","text":"old new record.event_name record.name record.event_source record.source record.event_source_arn record.source_arn","title":"SNS/SQS Record"},{"location":"install/","text":"Requirements \u00b6 Python 3.8 or higher; download and install Python Access to public python registry Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Install"},{"location":"install/#requirements","text":"Python 3.8 or higher; download and install Python Access to public python registry","title":"Requirements"},{"location":"install/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"apigateway/configuration-details/","text":"Apigateway Configuration Details \u00b6 If you are looking for a more customized application, you can review the full litany of configuration options below. This will give you the greatest control on how you python application will run. The router is the core of the apigateway event handler and will automatically route based on the way your project is configured. Most of the time, there is no need to manage lists of routes matched to files or exports; all that is required is that you create a file in the location or pattern configured to hold your endpoints and the router will automatically find it. Examples Don't like reading documentation? Then look at our examples, which can run locally! Lambda Configuration \u00b6 Serverless Framework 1 2 3 4 5 6 7 8 9 10 functions : apigateway-handler : handler : api/handler/router.route events : - http : path : / method : ANY - http : path : /{proxy+} method : ANY Router Configuration Options \u00b6 There are three ways to organize your routes: directory , pattern and mapping ; directory and pattern routing mode requires your project files to be placed in a particular way; mapping does not require any structure, as you define every route, and it's a corresponding file. Below are the three ways configure your router: Routing Mode: Directory \u00b6 Tip If you are using route params, you will need use dynamic file names which follow this pattern: _some_variable_name.py . file structure router.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ~~ Directory ~~ ~~ Route ~~ =================================================================== \ud83d\udce6api/ | \u2502---\ud83d\udcc2handler | \u2502---\ud83d\udcdcrouter.py | \u2502---\ud83d\udcdcorg.py | /org \u2502---\ud83d\udcc2grower | \u2502---\ud83d\udcdc__init__.py | /grower \u2502---\ud83d\udcdc_grower_id.py | /grower/{grower_id} \u2502---\ud83d\udcc2farm | \u2502---\ud83d\udcdc__init__.py | /farm \u2502---\ud83d\udcc2_farm_id | \u2502---\ud83d\udcdc__init__.py | /farm/{farm_id} \u2502---\ud83d\udcc2field | \u2502---\ud83d\udcdc__init__.py | /farm/{farm_id}/field \u2502---\ud83d\udcdc_field_id.py | /farm/{farm_id}/field/{field_id} 1 2 3 4 5 6 7 8 9 10 11 from acai_aws.apigateway.router import Router router = Router ( base_path = 'your-service/v1' , handlers = 'api/handlers' , schema = 'api/openapi.yml' ) router . auto_load () def handle ( event , context ): return router . route ( event , context ) Routing Mode: Pattern \u00b6 Tip You can use any glob pattern you like; common patterns are: /**/*_handler.py /**/handler_*.py /**/handler.py file structure router.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ~~ Pattern ~~ ~~ Route ~~ ================================================================================ \ud83d\udce6api/ | \u2502---\ud83d\udcdcrouter.py | \u2502---\ud83d\udcc2org | \u2502---\ud83d\udcdcorg_handler.py | /org \u2502---\ud83d\udcdcorg_model.py | \u2502---\ud83d\udcdcorg_factory.py | \u2502---\ud83d\udcdcorg_logic.py | \u2502---\ud83d\udcc2grower | \u2502---\ud83d\udcdcgrower_handler.py | /grower \u2502---\ud83d\udcdc_grower_id_handler.py | /grower/{grower_id} \u2502---\ud83d\udcdcgrower_model.py | \u2502---\ud83d\udcdcgrower_factory.py | \u2502---\ud83d\udcdcgrower_logic.py | \u2502---\ud83d\udcc2farm | \u2502---\ud83d\udcdcfarm_handler.py | /farm \u2502---\ud83d\udcdcfarm_logic.py | \u2502---\ud83d\udcdcfarm_model.py | \u2502---\ud83d\udcc2_farm_id | \u2502---\ud83d\udcdc_farm_id_handler.py | /farm/{farm_id} \u2502---\ud83d\udcc2field | \u2502---\ud83d\udcdcfield_handler.py | /farm/{farm_id}/field \u2502---\ud83d\udcdc_field_id_controller.py | /farm/{farm_id}/field/{field_id} \u2502---\ud83d\udcdcfield_logic.py | \u2502---\ud83d\udcdcfield_model.py | 1 2 3 4 5 6 7 8 9 10 11 from acai_aws.apigateway.router import Router router = Router ( base_path = 'your-service/v1' , handlers = 'api/*_handler.py' , schema = 'api/openapi.yml' ) router . auto_load () def handle ( event , context ): return router . route ( event , context ) Routing Mode: Mapping \u00b6 Tip It may be more maintainable to store your routes list in a separate file, this example does not have that for brevity Warning Even though you are matching your files to your routes, the handler files must have functions that match HTTP method (see endpoint examples here) Danger This is not the preferred routing mode to use; this can lead to a sloppy, unpredictable project architecture which will be hard to maintain and extend. This is NOT RECOMMENDED . file structure router.py 1 2 3 4 File structure doesn't matter ====================================================== \ud83d\udce6api/ \u2502---\ud83d\udcdcrouter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from acai_aws.apigateway.router import Router router = Router ( base_path = 'your-service/v1' , schema = 'api/openapi.yml' handlers = { 'grower' : 'api/routes/grower.py' , 'farm' : 'api/routes/farm.py' , 'farm/ {farm_id} /field/ {field_id} ' : 'api/routes/farm_field.py' } ) router . auto_load () def handle ( event , context ): return router . route ( event , context ) Full Router Configuration Options \u00b6 option type required description after_all func no will call this function after EVERY request to the API auto_validate bool no; requires schema will automatically validate request against openapi.yml base_path str yes the base path of the API Gateway instance this is running on before_all func no will call this function before EVERY request to the API cache_mode str no; 'all','static-only','dynamic-only' will cache route endpoint module (not response) based on option, all (default), static endpoints or dynamic endpoints (route with path variables) cache_size int no; (default 128) how many endpoint modules to cache handlers str yes file path pointing to the directory where the endpoints are on_error func no will call this function on every unhandled error; not including validation errors schema str yes, if after_all file path pointing to the location of the openapi.yml file with_auth func no will call this function when requirements decorator have auth_required set to True validate_response bool no will validate response of an endpoint, can effect performance, not recommended for production verbose_logging bool no will log every setup, every request and every response Endpoint Configuration Options \u00b6 Each endpoint is meant to be treated as a separate module within the API. These endpoints are not meant to be extended or commingled and thus should approach individually. If resources are meant to be shared across endpoints, then those resources should be packaged as shared classes or utilities. Every endpoint file should contain a function which matches an HTTP method in lower case. Most common are post , get , put , patch , delete , but this library does support custom methods, if you so choose. As long as the method of the request matches the function name, it will work. Each method within the endpoint file can have individual validation requirements. These requirements allow you to test all structural points of the request, with the ability to use JSONSchema and custom middleware to further extend the validation options. Info See the full configuration list, explanation and example of each setting in our Configurations Section . Tip If you are already using an openapi.yml , none of these requirements below are necessary. Ensure your router has enabled auto_validate with proper schema configured and the below requirements are not necessary for any basic structural validation (headers, body, query, params will be checked via openapi.yml). You can still use before , after & data_class with other custom validations for more advanced use cases. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 # example for endpoint file: api/grower.py from acai_aws.apigateway.requirements import requirements from api.logic.grower import Grower from api.logic.middlware import log_grower , filter_grower # example after function # def filter_grower(request, response, requirements): # if 'GET' in response.raw['message']: # print(response.raw) @requirements ( required_query = [ 'requester_id' ], available_query = [ 'grower_id' , 'grower_email' ], data_class = Grower , after = filter_grower , auth_required = True ) def get ( request , response ): response . body = { 'message' : 'GET called' , 'request_query_params' : request . query_params } return response # example before function # def log_grower(request, response, requirements): # print(request.body['grower_id']) @requirements ( required_body = 'v1-grower-post-request' , before = log_grower , auth_required = True ) def post ( request , response ): response . body = { 'message' : 'POST called' , 'request_body' : request . body } return response @requirements ( required_headers = [ 'x-api-key' , 'x-correlation-id' ] required_path = 'grower/ {grower_id} ' auth_required = True required_body = { 'type' : 'object' , 'required' : [ 'grower_id' ], 'additionalProperties' : False , 'properties' : { 'grower_id' : { 'type' : 'string' }, 'body' : { 'type' : 'object' }, 'dict' : { 'type' : 'boolean' } } } ) def patch ( request , response ): response . body = { 'message' : 'PATCH called' , 'request_body' : request . body } return response # requirements is not required def delete ( request , response ): response . body = { 'message' : 'DELETE called' } return response Full Requirements Configuration Options \u00b6 requirement type description required_headers array every header in this array must be in the headers of request available_headers array only headers in this array will be allowed in the request required_query array every item in the array is a required query string parameter available_query array only items in this array are allowed in the request required_path str when using parameters, this is the required parameters required_body str references a JSschema component in your schema required_auth bool will trigger with_auth function defined in the router config before func a custom function to be ran before your method function after func a custom function to be ran after your method function data_class class a custom class that will be passed instead of the request obj [ custom-requirement ] any see bottom of page required_headers \u00b6 Info Headers are case-sensitive, make sure your casing matches your expectations. 1 2 3 4 5 @requirements ( required_headers = [ 'x-api-key' ] ) def post ( request , response ): pass available_headers \u00b6 Warning This is not recommended for frequent use as it raises errors for every header which does not conform to the array provided. Many browsers, http tools, and libraries will automatically add headers to request, unbeknownst to the user. By using this setting, you will force every user of the endpoint to take extra care with the headers provided and may result in poor API consumer experience. 1 2 3 4 5 @requirements ( available_headers = [ 'x-api-key' , 'x-on-behalf-of' ] ) def post ( request , response ): pass required_query \u00b6 1 2 3 4 5 @requirements ( required_query = [ 'grower_id' ] ) def get ( request , response ): pass available_query \u00b6 Info available_query entries do NOT need to include entries already defined in the required_query ; what is required, is assumed to be available. 1 2 3 4 5 @requirements ( available_query = [ 'grower_email' ] ) def get ( request , response ): pass required_path \u00b6 Warning This is required if you are using dynamic routing (ex. _id.py ) with path parameters. The router will provide a path values in request.path_params 1 2 3 4 5 @requirements ( required_path = 'grower/ {id} ' ) def get ( request , response ): pass required_body \u00b6 Info This is referencing a components.schemas section of your openapi.yml file defined in the schema value in your router config, but you can also pass in a json schema in the form of a dict . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @requirements ( required_body = 'v1-grower-post-request' ) def post ( request , response ): pass @requirements ( required_body = { 'type' : 'object' , 'required' : [ 'grower_id' ], 'additionalProperties' : False , 'properties' : { 'grower_id' : { 'type' : 'string' }, 'body' : { 'type' : 'object' }, 'dict' : { 'type' : 'boolean' } } } ) def patch ( request , response ): pass auth_required \u00b6 Info This will trigger the function you provided in the router config under the with_auth configuration 1 2 3 4 5 @requirements ( auth_required = True ) def delete ( request , response ): pass before \u00b6 1 2 3 4 5 6 7 8 9 10 11 def before_func ( request , response , requirements ): print ( request ) print ( response ) print ( requirements ) @requirements ( before = before_func ) def post ( request , response ): pass after \u00b6 1 2 3 4 5 6 7 8 9 10 11 def after_func ( request , response , requirements ): print ( request ) print ( response ) print ( requirements ) @requirements ( before = after_func ) def post ( request , response ): pass data_class \u00b6 Info Instead of getting a request and response as arguments passed to your API function, you will get an instance of the class you provided here 1 2 3 4 5 6 7 8 9 10 class Grower : def __init__ ( self , request ): for k , v in request . body . items (): setattr ( self , k , v ) @requirements ( data_class = Grower ) def post ( grower , response ): pass custom requirements \u00b6 Info You can add as many custom requirements as you want, with any variable type you want, and they will be passed to your before_all , before , after_all , after and with_auth middleware defined functions. 1 2 3 4 5 @requirements ( your_custom_requirement = { 'whatever' : ( 'you' , 'want' )} ) def put ( request , response ): pass Request Object \u00b6 By default, every endpoint function will receive an instance of the Request class (aka request ) as the first argument of their function. This request has a lot of properties which will do common things automatically, but still allows the developer to override those operations if they deem necessary. Below is a list and examples of all the properties of the request : Request Properties \u00b6 property type mutable description method str no the http method of the request cookies list no the cookies of the request protocol str no the protocol of the request content_type str no the content_type of the request body host_url str no the host_url of the request was sent to domain str no the domain of the request was sent to stage str no the stage the lambda was deployed to resource str no the AWS resource being invoked authorizer object no if using a customized authorizer, the authorizer object headers object no the headers of the request params object no combination of query string and path params in one object query_params object no query string parameters from the request path_params object no the path parameters of the request route str no the requested route with placeholders of params path str no the raw requested path with actual param values json object no the body of the request, converted from json string in object xml object no the body of the request, converted from xml string in object graphql str no the body of the graphql request as a string body any no the body of the request, converted to based on data type raw any no the raw body of the request no conversion context object yes mutable request context to assigned and pass around event object no the full event originally coming from the lambda request.cookies \u00b6 1 2 3 4 print ( request . cookies ) # output: [ 'some-cookie' ] request.protocol \u00b6 1 2 3 4 print ( request . protocol ) # output: 'https' request.content_type \u00b6 1 2 3 4 print ( request . content_type ) # output: 'application/json' request.host_url \u00b6 1 2 3 4 print ( request . host_url ) # output: 'https://api.are-great.com' request.domain \u00b6 1 2 3 4 print ( request . domain ) # output: 'api.are-great.com' request.stage \u00b6 1 2 3 4 print ( request . stage ) # output: 'prod' request.method \u00b6 1 2 3 4 print ( request . method ) # output: 'get' request.resource \u00b6 1 2 3 4 print ( request . resource ) # output: '/{proxy+}' request.authorizer \u00b6 Tip This is only useful if you are using an external authorizer with your lambda. 1 2 3 4 5 6 7 8 9 10 print ( request . authorizer ) # output: { 'apiKey' : 'SOME KEY' , 'userId' : 'x-1-3-4' , 'correlationId' : 'abc12312' , 'principalId' : '9de3f415a97e410386dbef146e88744e' , 'integrationLatency' : 572 } request.headers \u00b6 1 2 3 4 5 6 7 print ( request . headers ) # output: { 'x-api-key' : 'SOME-KEY' , 'content-type' : 'application/json' } request.params \u00b6 Info This combines both path parameters and query string parameters, nested in one object. 1 2 3 4 5 6 7 8 9 10 11 print ( request . params ) # output: { 'query' : { 'name' : 'me' }, 'path' : { 'id' : 1 } } request.query_params \u00b6 1 2 3 4 5 6 print ( request . query_params ) # output: { 'name' : 'me' } request.path_params \u00b6 1 2 3 4 5 6 print ( request . path_params ) # output: { 'id' : 1 } request.route \u00b6 Info This will provide the route with the path param variables included 1 2 3 4 print ( request . route ) # output: 'grower/ {id} ' request.path \u00b6 Info This will provide the route with the path param values replacing the variables 1 2 3 4 print ( request . path ) # example output: 'grower/1' request.json \u00b6 Warning This will raise an unhandled exception if the body is not json compatible 1 2 3 4 5 6 print ( request . json ); # output: { 'some_json_key' : 'some_json_value' } 1 2 3 4 5 6 print ( request . form ); # output: { 'some_form_key' : 'some_form_value' } request.xml \u00b6 Warning This will raise an unhandled exception if the body is not xml compatible 1 2 3 4 5 6 python ( request . xml ); # output: { 'some_xml_key' : 'some_xml_value' } request.graphql \u00b6 Info This is graphql string since there is no object equivalent; you can pass this directly to your graphql resolver 1 2 3 4 5 6 7 8 python ( request . graphql ); # output: '{ players { name } } ' request.body \u00b6 Tip This is the safest way to get the body of the request. It will use the content-type header to determine the data sent and convert it; if the data can't be converted for whatever reason it will catch the error and return the raw body provided unconverted. 1 2 3 4 5 6 print ( request . body ) # output: { 'some_key' : 'some_value' } request.raw \u00b6 1 2 3 4 print ( request . raw ) # output: # whatever the raw data of the body is; string, json string, xml, binary, etc request.context \u00b6 Tip This is the only mutable property of the request, to be used by any of the before or before_all middleware options 1 2 3 4 5 6 7 request . context = { 'application_assignable' : true } print ( request . context ) # output: { 'application_assignable' : true } request.event \u00b6 Warning This is the original full request. Not advisable to use this as defeats the purpose of the entire Acai . In addition, you don't want to mutate this object and potentially mess up the entire router. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 print ( request . event ) # output: { \"version\" : \"2.0\" , \"routeKey\" : \"$default\" , \"rawPath\" : \"/my/path\" , \"rawQueryString\" : \"parameter1=value1&parameter1=value2&parameter2=value\" , \"cookies\" : [ \"cookie1\" , \"cookie2\" ], \"headers\" : { \"header1\" : \"value1\" , \"header2\" : \"value1,value2\" }, \"queryStringParameters\" : { \"parameter1\" : \"value1,value2\" , \"parameter2\" : \"value\" }, \"requestContext\" : { \"accountId\" : \"123456789012\" , \"apiId\" : \"api-id\" , \"authentication\" : { \"clientCert\" : { \"clientCertPem\" : \"CERT_CONTENT\" , \"subjectDN\" : \"www.example.com\" , \"issuerDN\" : \"Example issuer\" , \"serialNumber\" : \"a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1\" , \"validity\" : { \"notBefore\" : \"May 28 12:30:02 2019 GMT\" , \"notAfter\" : \"Aug 5 09:36:04 2021 GMT\" } } }, \"authorizer\" : { \"jwt\" : { \"claims\" : { \"claim1\" : \"value1\" , \"claim2\" : \"value2\" }, \"scopes\" : [ \"scope1\" , \"scope2\" ] } }, \"domainName\" : \"id.execute-api.us-east-1.amazonaws.com\" , \"domainPrefix\" : \"id\" , \"http\" : { \"method\" : \"POST\" , \"path\" : \"/my/path\" , \"protocol\" : \"HTTP/1.1\" , \"sourceIp\" : \"IP\" , \"userAgent\" : \"agent\" }, \"requestId\" : \"id\" , \"routeKey\" : \"$default\" , \"stage\" : \"$default\" , \"time\" : \"12/Mar/2020:19:03:58 +0000\" , \"timeEpoch\" : 1583348638390 }, \"body\" : \"Hello from Lambda\" , \"pathParameters\" : { \"parameter1\" : \"value1\" }, \"isBase64Encoded\" : False , \"stageVariables\" : { \"stageVariable1\" : \"value1\" , \"stageVariable2\" : \"value2\" } } Response Object \u00b6 By default, every endpoint function will receive an instance of the Response class (aka response ) as the second argument of their function. This response object is meant to provide consistency to HTTP response codes and error signatures. Below is a list and examples of all the properties of the response : Response Properties \u00b6 property type description headers tuple provide headers in tuple pairs to add new headers code int http response code to be returned the requester body any body of the response automatically converted to JSON string raw any body of the response not converted to JSON string compress bool will compress the body if set to true and add proper headers set_error func function to set an error with a key and value has_error boolean simple property to check if response already has errors in it response.headers \u00b6 1 2 3 4 5 6 7 8 9 10 response . headers = ( 'status' , 'ok' ) response . headers = ( 'response_id' , 'some-guid' ) print ( response . headers ) # output: { 'status' : 'ok' , 'response_id' : 'some-guid' , } response.code \u00b6 1 2 3 4 5 6 response . code = 418 ; print ( response . code ); # output: 418 response.body \u00b6 Info This will automatically convert the body to json if possible when called. 1 2 3 4 5 6 response . body = { 'some_key' : 'some_value' } print ( response . body ) # output: '{\"someKey\":\"someValue\"}' response.raw \u00b6 Info This will NOT automatically convert the body to json if possible when called. This is great when working with an after_all method that wants to mutate the body of the response before returning to the user. 1 2 3 4 5 6 7 8 response . raw = { 'some_key' : 'some_value' }; print ( response . raw ) # output: { 'some_key' : 'some_value' } response.compress \u00b6 Info This will compress whatever is in the body property. 1 2 3 4 response . compress = True print ( response . body ) # output: this will gzip and compress the body. response.set_error(key, value) \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 some_key = 'abc123' response . set_error ( 'someKey' , f ' { some_key } is not a valid key to use with this service; try again with a different key' ) another_key = 'def456' response . set_error ( 'anotherKey' , f ' { another_key } is not the correct type to operate on' ) print ( response . raw ) # output: { 'errors' : [ { 'key_path' : 'someKey' , 'message' : 'abc123 is not a valid key to use with this service; try again with a different key' }, { 'key_path' : 'anotherKey' , 'message' : 'def456 is not the correct type to operate on' } ] } response.has_error \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 response . setError ( 'user' , 'your access is denied' ) print ( response . has_error ) # output: True response . body = { 'user' : 'you have been granted access' }; print ( response . has_error ) # output: False","title":"Configuration Details"},{"location":"apigateway/configuration-details/#apigateway-configuration-details","text":"If you are looking for a more customized application, you can review the full litany of configuration options below. This will give you the greatest control on how you python application will run. The router is the core of the apigateway event handler and will automatically route based on the way your project is configured. Most of the time, there is no need to manage lists of routes matched to files or exports; all that is required is that you create a file in the location or pattern configured to hold your endpoints and the router will automatically find it. Examples Don't like reading documentation? Then look at our examples, which can run locally!","title":"Apigateway Configuration Details"},{"location":"apigateway/configuration-details/#lambda-configuration","text":"Serverless Framework 1 2 3 4 5 6 7 8 9 10 functions : apigateway-handler : handler : api/handler/router.route events : - http : path : / method : ANY - http : path : /{proxy+} method : ANY","title":"Lambda Configuration"},{"location":"apigateway/configuration-details/#router-configuration-options","text":"There are three ways to organize your routes: directory , pattern and mapping ; directory and pattern routing mode requires your project files to be placed in a particular way; mapping does not require any structure, as you define every route, and it's a corresponding file. Below are the three ways configure your router:","title":"Router Configuration Options"},{"location":"apigateway/configuration-details/#routing-mode-directory","text":"Tip If you are using route params, you will need use dynamic file names which follow this pattern: _some_variable_name.py . file structure router.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ~~ Directory ~~ ~~ Route ~~ =================================================================== \ud83d\udce6api/ | \u2502---\ud83d\udcc2handler | \u2502---\ud83d\udcdcrouter.py | \u2502---\ud83d\udcdcorg.py | /org \u2502---\ud83d\udcc2grower | \u2502---\ud83d\udcdc__init__.py | /grower \u2502---\ud83d\udcdc_grower_id.py | /grower/{grower_id} \u2502---\ud83d\udcc2farm | \u2502---\ud83d\udcdc__init__.py | /farm \u2502---\ud83d\udcc2_farm_id | \u2502---\ud83d\udcdc__init__.py | /farm/{farm_id} \u2502---\ud83d\udcc2field | \u2502---\ud83d\udcdc__init__.py | /farm/{farm_id}/field \u2502---\ud83d\udcdc_field_id.py | /farm/{farm_id}/field/{field_id} 1 2 3 4 5 6 7 8 9 10 11 from acai_aws.apigateway.router import Router router = Router ( base_path = 'your-service/v1' , handlers = 'api/handlers' , schema = 'api/openapi.yml' ) router . auto_load () def handle ( event , context ): return router . route ( event , context )","title":"Routing Mode: Directory"},{"location":"apigateway/configuration-details/#routing-mode-pattern","text":"Tip You can use any glob pattern you like; common patterns are: /**/*_handler.py /**/handler_*.py /**/handler.py file structure router.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ~~ Pattern ~~ ~~ Route ~~ ================================================================================ \ud83d\udce6api/ | \u2502---\ud83d\udcdcrouter.py | \u2502---\ud83d\udcc2org | \u2502---\ud83d\udcdcorg_handler.py | /org \u2502---\ud83d\udcdcorg_model.py | \u2502---\ud83d\udcdcorg_factory.py | \u2502---\ud83d\udcdcorg_logic.py | \u2502---\ud83d\udcc2grower | \u2502---\ud83d\udcdcgrower_handler.py | /grower \u2502---\ud83d\udcdc_grower_id_handler.py | /grower/{grower_id} \u2502---\ud83d\udcdcgrower_model.py | \u2502---\ud83d\udcdcgrower_factory.py | \u2502---\ud83d\udcdcgrower_logic.py | \u2502---\ud83d\udcc2farm | \u2502---\ud83d\udcdcfarm_handler.py | /farm \u2502---\ud83d\udcdcfarm_logic.py | \u2502---\ud83d\udcdcfarm_model.py | \u2502---\ud83d\udcc2_farm_id | \u2502---\ud83d\udcdc_farm_id_handler.py | /farm/{farm_id} \u2502---\ud83d\udcc2field | \u2502---\ud83d\udcdcfield_handler.py | /farm/{farm_id}/field \u2502---\ud83d\udcdc_field_id_controller.py | /farm/{farm_id}/field/{field_id} \u2502---\ud83d\udcdcfield_logic.py | \u2502---\ud83d\udcdcfield_model.py | 1 2 3 4 5 6 7 8 9 10 11 from acai_aws.apigateway.router import Router router = Router ( base_path = 'your-service/v1' , handlers = 'api/*_handler.py' , schema = 'api/openapi.yml' ) router . auto_load () def handle ( event , context ): return router . route ( event , context )","title":"Routing Mode: Pattern"},{"location":"apigateway/configuration-details/#routing-mode-mapping","text":"Tip It may be more maintainable to store your routes list in a separate file, this example does not have that for brevity Warning Even though you are matching your files to your routes, the handler files must have functions that match HTTP method (see endpoint examples here) Danger This is not the preferred routing mode to use; this can lead to a sloppy, unpredictable project architecture which will be hard to maintain and extend. This is NOT RECOMMENDED . file structure router.py 1 2 3 4 File structure doesn't matter ====================================================== \ud83d\udce6api/ \u2502---\ud83d\udcdcrouter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from acai_aws.apigateway.router import Router router = Router ( base_path = 'your-service/v1' , schema = 'api/openapi.yml' handlers = { 'grower' : 'api/routes/grower.py' , 'farm' : 'api/routes/farm.py' , 'farm/ {farm_id} /field/ {field_id} ' : 'api/routes/farm_field.py' } ) router . auto_load () def handle ( event , context ): return router . route ( event , context )","title":"Routing Mode: Mapping"},{"location":"apigateway/configuration-details/#full-router-configuration-options","text":"option type required description after_all func no will call this function after EVERY request to the API auto_validate bool no; requires schema will automatically validate request against openapi.yml base_path str yes the base path of the API Gateway instance this is running on before_all func no will call this function before EVERY request to the API cache_mode str no; 'all','static-only','dynamic-only' will cache route endpoint module (not response) based on option, all (default), static endpoints or dynamic endpoints (route with path variables) cache_size int no; (default 128) how many endpoint modules to cache handlers str yes file path pointing to the directory where the endpoints are on_error func no will call this function on every unhandled error; not including validation errors schema str yes, if after_all file path pointing to the location of the openapi.yml file with_auth func no will call this function when requirements decorator have auth_required set to True validate_response bool no will validate response of an endpoint, can effect performance, not recommended for production verbose_logging bool no will log every setup, every request and every response","title":"Full Router Configuration Options"},{"location":"apigateway/configuration-details/#endpoint-configuration-options","text":"Each endpoint is meant to be treated as a separate module within the API. These endpoints are not meant to be extended or commingled and thus should approach individually. If resources are meant to be shared across endpoints, then those resources should be packaged as shared classes or utilities. Every endpoint file should contain a function which matches an HTTP method in lower case. Most common are post , get , put , patch , delete , but this library does support custom methods, if you so choose. As long as the method of the request matches the function name, it will work. Each method within the endpoint file can have individual validation requirements. These requirements allow you to test all structural points of the request, with the ability to use JSONSchema and custom middleware to further extend the validation options. Info See the full configuration list, explanation and example of each setting in our Configurations Section . Tip If you are already using an openapi.yml , none of these requirements below are necessary. Ensure your router has enabled auto_validate with proper schema configured and the below requirements are not necessary for any basic structural validation (headers, body, query, params will be checked via openapi.yml). You can still use before , after & data_class with other custom validations for more advanced use cases. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 # example for endpoint file: api/grower.py from acai_aws.apigateway.requirements import requirements from api.logic.grower import Grower from api.logic.middlware import log_grower , filter_grower # example after function # def filter_grower(request, response, requirements): # if 'GET' in response.raw['message']: # print(response.raw) @requirements ( required_query = [ 'requester_id' ], available_query = [ 'grower_id' , 'grower_email' ], data_class = Grower , after = filter_grower , auth_required = True ) def get ( request , response ): response . body = { 'message' : 'GET called' , 'request_query_params' : request . query_params } return response # example before function # def log_grower(request, response, requirements): # print(request.body['grower_id']) @requirements ( required_body = 'v1-grower-post-request' , before = log_grower , auth_required = True ) def post ( request , response ): response . body = { 'message' : 'POST called' , 'request_body' : request . body } return response @requirements ( required_headers = [ 'x-api-key' , 'x-correlation-id' ] required_path = 'grower/ {grower_id} ' auth_required = True required_body = { 'type' : 'object' , 'required' : [ 'grower_id' ], 'additionalProperties' : False , 'properties' : { 'grower_id' : { 'type' : 'string' }, 'body' : { 'type' : 'object' }, 'dict' : { 'type' : 'boolean' } } } ) def patch ( request , response ): response . body = { 'message' : 'PATCH called' , 'request_body' : request . body } return response # requirements is not required def delete ( request , response ): response . body = { 'message' : 'DELETE called' } return response","title":"Endpoint Configuration Options"},{"location":"apigateway/configuration-details/#full-requirements-configuration-options","text":"requirement type description required_headers array every header in this array must be in the headers of request available_headers array only headers in this array will be allowed in the request required_query array every item in the array is a required query string parameter available_query array only items in this array are allowed in the request required_path str when using parameters, this is the required parameters required_body str references a JSschema component in your schema required_auth bool will trigger with_auth function defined in the router config before func a custom function to be ran before your method function after func a custom function to be ran after your method function data_class class a custom class that will be passed instead of the request obj [ custom-requirement ] any see bottom of page","title":"Full Requirements Configuration Options"},{"location":"apigateway/configuration-details/#required_headers","text":"Info Headers are case-sensitive, make sure your casing matches your expectations. 1 2 3 4 5 @requirements ( required_headers = [ 'x-api-key' ] ) def post ( request , response ): pass","title":"required_headers"},{"location":"apigateway/configuration-details/#available_headers","text":"Warning This is not recommended for frequent use as it raises errors for every header which does not conform to the array provided. Many browsers, http tools, and libraries will automatically add headers to request, unbeknownst to the user. By using this setting, you will force every user of the endpoint to take extra care with the headers provided and may result in poor API consumer experience. 1 2 3 4 5 @requirements ( available_headers = [ 'x-api-key' , 'x-on-behalf-of' ] ) def post ( request , response ): pass","title":"available_headers"},{"location":"apigateway/configuration-details/#required_query","text":"1 2 3 4 5 @requirements ( required_query = [ 'grower_id' ] ) def get ( request , response ): pass","title":"required_query"},{"location":"apigateway/configuration-details/#available_query","text":"Info available_query entries do NOT need to include entries already defined in the required_query ; what is required, is assumed to be available. 1 2 3 4 5 @requirements ( available_query = [ 'grower_email' ] ) def get ( request , response ): pass","title":"available_query"},{"location":"apigateway/configuration-details/#required_path","text":"Warning This is required if you are using dynamic routing (ex. _id.py ) with path parameters. The router will provide a path values in request.path_params 1 2 3 4 5 @requirements ( required_path = 'grower/ {id} ' ) def get ( request , response ): pass","title":"required_path"},{"location":"apigateway/configuration-details/#required_body","text":"Info This is referencing a components.schemas section of your openapi.yml file defined in the schema value in your router config, but you can also pass in a json schema in the form of a dict . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @requirements ( required_body = 'v1-grower-post-request' ) def post ( request , response ): pass @requirements ( required_body = { 'type' : 'object' , 'required' : [ 'grower_id' ], 'additionalProperties' : False , 'properties' : { 'grower_id' : { 'type' : 'string' }, 'body' : { 'type' : 'object' }, 'dict' : { 'type' : 'boolean' } } } ) def patch ( request , response ): pass","title":"required_body"},{"location":"apigateway/configuration-details/#auth_required","text":"Info This will trigger the function you provided in the router config under the with_auth configuration 1 2 3 4 5 @requirements ( auth_required = True ) def delete ( request , response ): pass","title":"auth_required"},{"location":"apigateway/configuration-details/#before","text":"1 2 3 4 5 6 7 8 9 10 11 def before_func ( request , response , requirements ): print ( request ) print ( response ) print ( requirements ) @requirements ( before = before_func ) def post ( request , response ): pass","title":"before"},{"location":"apigateway/configuration-details/#after","text":"1 2 3 4 5 6 7 8 9 10 11 def after_func ( request , response , requirements ): print ( request ) print ( response ) print ( requirements ) @requirements ( before = after_func ) def post ( request , response ): pass","title":"after"},{"location":"apigateway/configuration-details/#data_class","text":"Info Instead of getting a request and response as arguments passed to your API function, you will get an instance of the class you provided here 1 2 3 4 5 6 7 8 9 10 class Grower : def __init__ ( self , request ): for k , v in request . body . items (): setattr ( self , k , v ) @requirements ( data_class = Grower ) def post ( grower , response ): pass","title":"data_class"},{"location":"apigateway/configuration-details/#custom-requirements","text":"Info You can add as many custom requirements as you want, with any variable type you want, and they will be passed to your before_all , before , after_all , after and with_auth middleware defined functions. 1 2 3 4 5 @requirements ( your_custom_requirement = { 'whatever' : ( 'you' , 'want' )} ) def put ( request , response ): pass","title":"custom requirements"},{"location":"apigateway/configuration-details/#request-object","text":"By default, every endpoint function will receive an instance of the Request class (aka request ) as the first argument of their function. This request has a lot of properties which will do common things automatically, but still allows the developer to override those operations if they deem necessary. Below is a list and examples of all the properties of the request :","title":"Request Object"},{"location":"apigateway/configuration-details/#request-properties","text":"property type mutable description method str no the http method of the request cookies list no the cookies of the request protocol str no the protocol of the request content_type str no the content_type of the request body host_url str no the host_url of the request was sent to domain str no the domain of the request was sent to stage str no the stage the lambda was deployed to resource str no the AWS resource being invoked authorizer object no if using a customized authorizer, the authorizer object headers object no the headers of the request params object no combination of query string and path params in one object query_params object no query string parameters from the request path_params object no the path parameters of the request route str no the requested route with placeholders of params path str no the raw requested path with actual param values json object no the body of the request, converted from json string in object xml object no the body of the request, converted from xml string in object graphql str no the body of the graphql request as a string body any no the body of the request, converted to based on data type raw any no the raw body of the request no conversion context object yes mutable request context to assigned and pass around event object no the full event originally coming from the lambda","title":"Request Properties"},{"location":"apigateway/configuration-details/#requestcookies","text":"1 2 3 4 print ( request . cookies ) # output: [ 'some-cookie' ]","title":"request.cookies"},{"location":"apigateway/configuration-details/#requestprotocol","text":"1 2 3 4 print ( request . protocol ) # output: 'https'","title":"request.protocol"},{"location":"apigateway/configuration-details/#requestcontent_type","text":"1 2 3 4 print ( request . content_type ) # output: 'application/json'","title":"request.content_type"},{"location":"apigateway/configuration-details/#requesthost_url","text":"1 2 3 4 print ( request . host_url ) # output: 'https://api.are-great.com'","title":"request.host_url"},{"location":"apigateway/configuration-details/#requestdomain","text":"1 2 3 4 print ( request . domain ) # output: 'api.are-great.com'","title":"request.domain"},{"location":"apigateway/configuration-details/#requeststage","text":"1 2 3 4 print ( request . stage ) # output: 'prod'","title":"request.stage"},{"location":"apigateway/configuration-details/#requestmethod","text":"1 2 3 4 print ( request . method ) # output: 'get'","title":"request.method"},{"location":"apigateway/configuration-details/#requestresource","text":"1 2 3 4 print ( request . resource ) # output: '/{proxy+}'","title":"request.resource"},{"location":"apigateway/configuration-details/#requestauthorizer","text":"Tip This is only useful if you are using an external authorizer with your lambda. 1 2 3 4 5 6 7 8 9 10 print ( request . authorizer ) # output: { 'apiKey' : 'SOME KEY' , 'userId' : 'x-1-3-4' , 'correlationId' : 'abc12312' , 'principalId' : '9de3f415a97e410386dbef146e88744e' , 'integrationLatency' : 572 }","title":"request.authorizer"},{"location":"apigateway/configuration-details/#requestheaders","text":"1 2 3 4 5 6 7 print ( request . headers ) # output: { 'x-api-key' : 'SOME-KEY' , 'content-type' : 'application/json' }","title":"request.headers"},{"location":"apigateway/configuration-details/#requestparams","text":"Info This combines both path parameters and query string parameters, nested in one object. 1 2 3 4 5 6 7 8 9 10 11 print ( request . params ) # output: { 'query' : { 'name' : 'me' }, 'path' : { 'id' : 1 } }","title":"request.params"},{"location":"apigateway/configuration-details/#requestquery_params","text":"1 2 3 4 5 6 print ( request . query_params ) # output: { 'name' : 'me' }","title":"request.query_params"},{"location":"apigateway/configuration-details/#requestpath_params","text":"1 2 3 4 5 6 print ( request . path_params ) # output: { 'id' : 1 }","title":"request.path_params"},{"location":"apigateway/configuration-details/#requestroute","text":"Info This will provide the route with the path param variables included 1 2 3 4 print ( request . route ) # output: 'grower/ {id} '","title":"request.route"},{"location":"apigateway/configuration-details/#requestpath","text":"Info This will provide the route with the path param values replacing the variables 1 2 3 4 print ( request . path ) # example output: 'grower/1'","title":"request.path"},{"location":"apigateway/configuration-details/#requestjson","text":"Warning This will raise an unhandled exception if the body is not json compatible 1 2 3 4 5 6 print ( request . json ); # output: { 'some_json_key' : 'some_json_value' } 1 2 3 4 5 6 print ( request . form ); # output: { 'some_form_key' : 'some_form_value' }","title":"request.json"},{"location":"apigateway/configuration-details/#requestxml","text":"Warning This will raise an unhandled exception if the body is not xml compatible 1 2 3 4 5 6 python ( request . xml ); # output: { 'some_xml_key' : 'some_xml_value' }","title":"request.xml"},{"location":"apigateway/configuration-details/#requestgraphql","text":"Info This is graphql string since there is no object equivalent; you can pass this directly to your graphql resolver 1 2 3 4 5 6 7 8 python ( request . graphql ); # output: '{ players { name } } '","title":"request.graphql"},{"location":"apigateway/configuration-details/#requestbody","text":"Tip This is the safest way to get the body of the request. It will use the content-type header to determine the data sent and convert it; if the data can't be converted for whatever reason it will catch the error and return the raw body provided unconverted. 1 2 3 4 5 6 print ( request . body ) # output: { 'some_key' : 'some_value' }","title":"request.body"},{"location":"apigateway/configuration-details/#requestraw","text":"1 2 3 4 print ( request . raw ) # output: # whatever the raw data of the body is; string, json string, xml, binary, etc","title":"request.raw"},{"location":"apigateway/configuration-details/#requestcontext","text":"Tip This is the only mutable property of the request, to be used by any of the before or before_all middleware options 1 2 3 4 5 6 7 request . context = { 'application_assignable' : true } print ( request . context ) # output: { 'application_assignable' : true }","title":"request.context"},{"location":"apigateway/configuration-details/#requestevent","text":"Warning This is the original full request. Not advisable to use this as defeats the purpose of the entire Acai . In addition, you don't want to mutate this object and potentially mess up the entire router. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 print ( request . event ) # output: { \"version\" : \"2.0\" , \"routeKey\" : \"$default\" , \"rawPath\" : \"/my/path\" , \"rawQueryString\" : \"parameter1=value1&parameter1=value2&parameter2=value\" , \"cookies\" : [ \"cookie1\" , \"cookie2\" ], \"headers\" : { \"header1\" : \"value1\" , \"header2\" : \"value1,value2\" }, \"queryStringParameters\" : { \"parameter1\" : \"value1,value2\" , \"parameter2\" : \"value\" }, \"requestContext\" : { \"accountId\" : \"123456789012\" , \"apiId\" : \"api-id\" , \"authentication\" : { \"clientCert\" : { \"clientCertPem\" : \"CERT_CONTENT\" , \"subjectDN\" : \"www.example.com\" , \"issuerDN\" : \"Example issuer\" , \"serialNumber\" : \"a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1\" , \"validity\" : { \"notBefore\" : \"May 28 12:30:02 2019 GMT\" , \"notAfter\" : \"Aug 5 09:36:04 2021 GMT\" } } }, \"authorizer\" : { \"jwt\" : { \"claims\" : { \"claim1\" : \"value1\" , \"claim2\" : \"value2\" }, \"scopes\" : [ \"scope1\" , \"scope2\" ] } }, \"domainName\" : \"id.execute-api.us-east-1.amazonaws.com\" , \"domainPrefix\" : \"id\" , \"http\" : { \"method\" : \"POST\" , \"path\" : \"/my/path\" , \"protocol\" : \"HTTP/1.1\" , \"sourceIp\" : \"IP\" , \"userAgent\" : \"agent\" }, \"requestId\" : \"id\" , \"routeKey\" : \"$default\" , \"stage\" : \"$default\" , \"time\" : \"12/Mar/2020:19:03:58 +0000\" , \"timeEpoch\" : 1583348638390 }, \"body\" : \"Hello from Lambda\" , \"pathParameters\" : { \"parameter1\" : \"value1\" }, \"isBase64Encoded\" : False , \"stageVariables\" : { \"stageVariable1\" : \"value1\" , \"stageVariable2\" : \"value2\" } }","title":"request.event"},{"location":"apigateway/configuration-details/#response-object","text":"By default, every endpoint function will receive an instance of the Response class (aka response ) as the second argument of their function. This response object is meant to provide consistency to HTTP response codes and error signatures. Below is a list and examples of all the properties of the response :","title":"Response Object"},{"location":"apigateway/configuration-details/#response-properties","text":"property type description headers tuple provide headers in tuple pairs to add new headers code int http response code to be returned the requester body any body of the response automatically converted to JSON string raw any body of the response not converted to JSON string compress bool will compress the body if set to true and add proper headers set_error func function to set an error with a key and value has_error boolean simple property to check if response already has errors in it","title":"Response Properties"},{"location":"apigateway/configuration-details/#responseheaders","text":"1 2 3 4 5 6 7 8 9 10 response . headers = ( 'status' , 'ok' ) response . headers = ( 'response_id' , 'some-guid' ) print ( response . headers ) # output: { 'status' : 'ok' , 'response_id' : 'some-guid' , }","title":"response.headers"},{"location":"apigateway/configuration-details/#responsecode","text":"1 2 3 4 5 6 response . code = 418 ; print ( response . code ); # output: 418","title":"response.code"},{"location":"apigateway/configuration-details/#responsebody","text":"Info This will automatically convert the body to json if possible when called. 1 2 3 4 5 6 response . body = { 'some_key' : 'some_value' } print ( response . body ) # output: '{\"someKey\":\"someValue\"}'","title":"response.body"},{"location":"apigateway/configuration-details/#responseraw","text":"Info This will NOT automatically convert the body to json if possible when called. This is great when working with an after_all method that wants to mutate the body of the response before returning to the user. 1 2 3 4 5 6 7 8 response . raw = { 'some_key' : 'some_value' }; print ( response . raw ) # output: { 'some_key' : 'some_value' }","title":"response.raw"},{"location":"apigateway/configuration-details/#responsecompress","text":"Info This will compress whatever is in the body property. 1 2 3 4 response . compress = True print ( response . body ) # output: this will gzip and compress the body.","title":"response.compress"},{"location":"apigateway/configuration-details/#responseset_errorkey-value","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 some_key = 'abc123' response . set_error ( 'someKey' , f ' { some_key } is not a valid key to use with this service; try again with a different key' ) another_key = 'def456' response . set_error ( 'anotherKey' , f ' { another_key } is not the correct type to operate on' ) print ( response . raw ) # output: { 'errors' : [ { 'key_path' : 'someKey' , 'message' : 'abc123 is not a valid key to use with this service; try again with a different key' }, { 'key_path' : 'anotherKey' , 'message' : 'def456 is not the correct type to operate on' } ] }","title":"response.set_error(key, value)"},{"location":"apigateway/configuration-details/#responsehas_error","text":"1 2 3 4 5 6 7 8 9 10 11 12 response . setError ( 'user' , 'your access is denied' ) print ( response . has_error ) # output: True response . body = { 'user' : 'you have been granted access' }; print ( response . has_error ) # output: False","title":"response.has_error"},{"location":"apigateway/quickstart/","text":"Apigateway Quickstart \u00b6 Event handler and router for Amazon APIGateway REST & GraphQL, allowing you to take advantage of procesing all api requests in one lambda. Features \u00b6 Configurable router based on 3 common routing patterns Built-in request validation using standard OpenAPI schema Easily validate request in modular and declarative way without any additional code Able to easily extend with custom middleware at both app and per-endpoint levels Support for CORS, binary and Gzip compression Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. For the full set of configurations, please review the full list of configurations found router setup page. router.py serverless.yml file structure 1 2 3 4 5 6 7 8 9 10 11 from acai_aws.apigateway.router import Router router = Router ( base_path = 'your-service/v1' , handlers = 'api/handlers' , schema = 'api/openapi.yml' ) router . auto_load () def handle ( event , context ): return router . route ( event , context ) 1 2 3 4 5 6 7 8 9 10 functions : apigateway-handler : handler : api/handlers/router.handle events : - http : path : / method : ANY - http : path : /{proxy+} method : ANY 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ~~ Directory ~~ ~~ Route ~~ =================================================================== \ud83d\udce6api/ | \u2502---\ud83d\udcc2handlers | \u2502---\ud83d\udcdcrouter.py | \u2502---\ud83d\udcdcorg.py | /org \u2502---\ud83d\udcc2grower | \u2502---\ud83d\udcdc__init__.py | /grower \u2502---\ud83d\udcdc_grower_id.py | /grower/{grower_id} \u2502---\ud83d\udcc2farm | \u2502---\ud83d\udcdc__init__.py | /farm \u2502---\ud83d\udcc2_farm_id | \u2502---\ud83d\udcdc__init__.py | /farm/{farm_id} \u2502---\ud83d\udcc2field | \u2502---\ud83d\udcdc__init__.py | /farm/{farm_id}/field \u2502---\ud83d\udcdc_field_id.py | /farm/{farm_id}/field/{field_id}","title":"Quickstart"},{"location":"apigateway/quickstart/#apigateway-quickstart","text":"Event handler and router for Amazon APIGateway REST & GraphQL, allowing you to take advantage of procesing all api requests in one lambda.","title":"Apigateway Quickstart"},{"location":"apigateway/quickstart/#features","text":"Configurable router based on 3 common routing patterns Built-in request validation using standard OpenAPI schema Easily validate request in modular and declarative way without any additional code Able to easily extend with custom middleware at both app and per-endpoint levels Support for CORS, binary and Gzip compression","title":"Features"},{"location":"apigateway/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"apigateway/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. For the full set of configurations, please review the full list of configurations found router setup page. router.py serverless.yml file structure 1 2 3 4 5 6 7 8 9 10 11 from acai_aws.apigateway.router import Router router = Router ( base_path = 'your-service/v1' , handlers = 'api/handlers' , schema = 'api/openapi.yml' ) router . auto_load () def handle ( event , context ): return router . route ( event , context ) 1 2 3 4 5 6 7 8 9 10 functions : apigateway-handler : handler : api/handlers/router.handle events : - http : path : / method : ANY - http : path : /{proxy+} method : ANY 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ~~ Directory ~~ ~~ Route ~~ =================================================================== \ud83d\udce6api/ | \u2502---\ud83d\udcc2handlers | \u2502---\ud83d\udcdcrouter.py | \u2502---\ud83d\udcdcorg.py | /org \u2502---\ud83d\udcc2grower | \u2502---\ud83d\udcdc__init__.py | /grower \u2502---\ud83d\udcdc_grower_id.py | /grower/{grower_id} \u2502---\ud83d\udcc2farm | \u2502---\ud83d\udcdc__init__.py | /farm \u2502---\ud83d\udcc2_farm_id | \u2502---\ud83d\udcdc__init__.py | /farm/{farm_id} \u2502---\ud83d\udcc2field | \u2502---\ud83d\udcdc__init__.py | /farm/{farm_id}/field \u2502---\ud83d\udcdc_field_id.py | /farm/{farm_id}/field/{field_id}","title":"Minimal Setup"},{"location":"common/logger/","text":"The Acai logger is automatically logs in a formatted JSON string for easy reading and searching with AWS Cloud Watch. A developer can then use AWS filter patterns making it effortless to find the exact log they are looking for. Below is an example of how to use the logger: Basic Usage \u00b6 1 2 3 4 5 6 7 8 9 10 11 from acai_aws.common import logger logger . log ( level = 'INFO' , log = 'some log' ) # level=INFO|DEBUG|WARN|ERROR # exammple output \"\"\" { level: '$LEVEL', log: '$MESSGE' } \"\"\"","title":"Logger"},{"location":"common/logger/#basic-usage","text":"1 2 3 4 5 6 7 8 9 10 11 from acai_aws.common import logger logger . log ( level = 'INFO' , log = 'some log' ) # level=INFO|DEBUG|WARN|ERROR # exammple output \"\"\" { level: '$LEVEL', log: '$MESSGE' } \"\"\"","title":"Basic Usage"},{"location":"documentdb/configuration-details/","text":"DocumentDB Configurations Details \u00b6 The DocumentDB event will automatically handle many common things done when eventing off a DocumentDB stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : documentdb-handler : handler : service/handlers/documentdb.handle memorySize : 512 timeout : 30 events : - stream : type : documentdb arn : Fn::GetAtt : [ SomeDocDBCluser , ClusterArn ] Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object operations list no ['created', 'updated', 'deleted'] will only run if record was created from the listed operation raise_operation_error bool no False will raise exception if operation of record is not from listed operations raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from acai_aws.documentdb.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , operations = [ 'created' , 'deleted' , 'updated' ], data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-docdb-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record ) DocumentDB Record Properties \u00b6 property type description body object the new image of dynamodb record; created or updated record.region \u00b6 1 2 3 4 print ( record . region ); # output 'us-east-2'","title":"Configurations Details"},{"location":"documentdb/configuration-details/#documentdb-configurations-details","text":"The DocumentDB event will automatically handle many common things done when eventing off a DocumentDB stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"DocumentDB Configurations Details"},{"location":"documentdb/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : documentdb-handler : handler : service/handlers/documentdb.handle memorySize : 512 timeout : 30 events : - stream : type : documentdb arn : Fn::GetAtt : [ SomeDocDBCluser , ClusterArn ]","title":"Lambda Configuration"},{"location":"documentdb/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object operations list no ['created', 'updated', 'deleted'] will only run if record was created from the listed operation raise_operation_error bool no False will raise exception if operation of record is not from listed operations raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from acai_aws.documentdb.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , operations = [ 'created' , 'deleted' , 'updated' ], data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-docdb-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record )","title":"Requirements Configuration Options"},{"location":"documentdb/configuration-details/#documentdb-record-properties","text":"property type description body object the new image of dynamodb record; created or updated","title":"DocumentDB Record Properties"},{"location":"documentdb/configuration-details/#recordregion","text":"1 2 3 4 print ( record . region ); # output 'us-east-2'","title":"record.region"},{"location":"documentdb/quickstart/","text":"DocumentDB Quickstart \u00b6 Event handler for Amazon DocumentDB Stream Events. Features \u00b6 Automatically convert DocumentDB IDs to standard strings Only run on certain DocumentDB operations; i.e. when documents are created, updated or deleted Able to validate DocumentDB document against a JSON Schema Assign Data Classes to records instead of getting DocumentDB BJSON dicts Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. documentdb.py serverless.yml 1 2 3 4 5 6 7 8 from acai_aws.documentdb.requirements import requirements @requirements ( operations = [ 'created' , 'deleted' ] ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 10 functions : documentdb-handler : handler : service/handlers/documentdb.handle memorySize : 512 timeout : 30 events : - stream : type : documentdb arn : Fn::GetAtt : [ SomeDocDBCluser , ClusterArn ]","title":"Quickstart"},{"location":"documentdb/quickstart/#documentdb-quickstart","text":"Event handler for Amazon DocumentDB Stream Events.","title":"DocumentDB Quickstart"},{"location":"documentdb/quickstart/#features","text":"Automatically convert DocumentDB IDs to standard strings Only run on certain DocumentDB operations; i.e. when documents are created, updated or deleted Able to validate DocumentDB document against a JSON Schema Assign Data Classes to records instead of getting DocumentDB BJSON dicts","title":"Features"},{"location":"documentdb/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"documentdb/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. documentdb.py serverless.yml 1 2 3 4 5 6 7 8 from acai_aws.documentdb.requirements import requirements @requirements ( operations = [ 'created' , 'deleted' ] ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 10 functions : documentdb-handler : handler : service/handlers/documentdb.handle memorySize : 512 timeout : 30 events : - stream : type : documentdb arn : Fn::GetAtt : [ SomeDocDBCluser , ClusterArn ]","title":"Minimal Setup"},{"location":"dynamodb/configuration-details/","text":"DynamoDB Configurations Details \u00b6 The Dynamodb event will automatically handle many common things done when eventing off a DynamoDB stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : ddb-handler : handler : service/handlers/dynamodb.handle memorySize : 512 timeout : 30 events : - stream : type : dynamodb arn : Fn::GetAtt : [ SomeDDBTable , StreamArn ] Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object operations list no ['created', 'updated', 'deleted'] will only run if record was created from the listed operation raise_operation_error bool no False will raise exception if operation of record is not from listed operations raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from acai_aws.dynamodb.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , operations = [ 'created' , 'deleted' , 'updated' ], data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-ddb-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record ) DynamoDB Record Properties \u00b6 property type description body object the new image of dynamodb record; created or updated created float the approximate creationDate time expired bool whether the ttl has expired id str the id of the event which invoked the lambda identity object the identity who triggered the dynamodb change keys object the keys of DynamoDB record name str the name of the event which invoked the lambda new_image object the new image of dynamodb record; created or updated old_image object the old image of dynamodb record; updated or deleted operation str triggered operation lambda (create, update, delete) region str the region the record is from size int the size in bytes of the record source str the source of the event which invoked the lambda source_arn str the event source arn stream_type str the stream view type version str the event version operation str enum of created , deleted , updated record.region \u00b6 1 2 3 4 print ( record . region ); # output 'us-east-2' record.id \u00b6 1 2 3 4 print ( record . id ); # output '9a37c0d03eb60f7cf70cabc823de9907' record.name \u00b6 1 2 3 4 print ( record . name ); # output 'INSERT' record.source \u00b6 1 2 3 4 print ( record . source ); # output 'aws:dynamodb' record.keys \u00b6 Info This is converted from the original DDB JSON to standard json 1 2 3 4 5 6 print ( record . keys ); # output { 'example_id' : '123456789' } record.old_image \u00b6 Info This is converted from the original DDB JSON to standard json 1 2 3 4 5 6 print ( record . old_image ); # output { 'old_data' : '123456789' } record.new_image \u00b6 Info This is converted from the original DDB JSON to standard json 1 2 3 4 5 6 print ( record . new_image ); # output { 'new_data' : '123456789' } record.body \u00b6 Info This is converted from the original DDB JSON to standard json from new_image 1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' } record.operation \u00b6 1 2 3 4 print ( record . operation ); # output 'create' record.source_arn \u00b6 1 2 3 4 print ( record . source_arn ); # output 'arn:aws:dynamodb:us-east-1:771875143460:table/test-example/stream/2019-10-04T23:18:26.340' record.version \u00b6 1 2 3 4 print ( record . version ); # output '1.1' record.stream_type \u00b6 1 2 3 4 print ( record . stream_type ); # output 'NEW_AND_OLD_IMAGES' record.size \u00b6 1 2 3 4 print ( record . size ); # output 1124 record.created \u00b6 1 2 3 4 print ( record . created ); # output 1538695200.0 #unix timestamp record.identity \u00b6 1 2 3 4 5 6 7 print ( record . identity ); # output { 'type' : 'Service' , 'principalId' : 'dynamodb.amazonaws.com' } record.expired \u00b6 1 2 3 4 print ( record . expired ); # output False","title":"Configurations Details"},{"location":"dynamodb/configuration-details/#dynamodb-configurations-details","text":"The Dynamodb event will automatically handle many common things done when eventing off a DynamoDB stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"DynamoDB Configurations Details"},{"location":"dynamodb/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : ddb-handler : handler : service/handlers/dynamodb.handle memorySize : 512 timeout : 30 events : - stream : type : dynamodb arn : Fn::GetAtt : [ SomeDDBTable , StreamArn ]","title":"Lambda Configuration"},{"location":"dynamodb/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object operations list no ['created', 'updated', 'deleted'] will only run if record was created from the listed operation raise_operation_error bool no False will raise exception if operation of record is not from listed operations raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from acai_aws.dynamodb.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , operations = [ 'created' , 'deleted' , 'updated' ], data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-ddb-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record )","title":"Requirements Configuration Options"},{"location":"dynamodb/configuration-details/#dynamodb-record-properties","text":"property type description body object the new image of dynamodb record; created or updated created float the approximate creationDate time expired bool whether the ttl has expired id str the id of the event which invoked the lambda identity object the identity who triggered the dynamodb change keys object the keys of DynamoDB record name str the name of the event which invoked the lambda new_image object the new image of dynamodb record; created or updated old_image object the old image of dynamodb record; updated or deleted operation str triggered operation lambda (create, update, delete) region str the region the record is from size int the size in bytes of the record source str the source of the event which invoked the lambda source_arn str the event source arn stream_type str the stream view type version str the event version operation str enum of created , deleted , updated","title":"DynamoDB Record Properties"},{"location":"dynamodb/configuration-details/#recordregion","text":"1 2 3 4 print ( record . region ); # output 'us-east-2'","title":"record.region"},{"location":"dynamodb/configuration-details/#recordid","text":"1 2 3 4 print ( record . id ); # output '9a37c0d03eb60f7cf70cabc823de9907'","title":"record.id"},{"location":"dynamodb/configuration-details/#recordname","text":"1 2 3 4 print ( record . name ); # output 'INSERT'","title":"record.name"},{"location":"dynamodb/configuration-details/#recordsource","text":"1 2 3 4 print ( record . source ); # output 'aws:dynamodb'","title":"record.source"},{"location":"dynamodb/configuration-details/#recordkeys","text":"Info This is converted from the original DDB JSON to standard json 1 2 3 4 5 6 print ( record . keys ); # output { 'example_id' : '123456789' }","title":"record.keys"},{"location":"dynamodb/configuration-details/#recordold_image","text":"Info This is converted from the original DDB JSON to standard json 1 2 3 4 5 6 print ( record . old_image ); # output { 'old_data' : '123456789' }","title":"record.old_image"},{"location":"dynamodb/configuration-details/#recordnew_image","text":"Info This is converted from the original DDB JSON to standard json 1 2 3 4 5 6 print ( record . new_image ); # output { 'new_data' : '123456789' }","title":"record.new_image"},{"location":"dynamodb/configuration-details/#recordbody","text":"Info This is converted from the original DDB JSON to standard json from new_image 1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' }","title":"record.body"},{"location":"dynamodb/configuration-details/#recordoperation","text":"1 2 3 4 print ( record . operation ); # output 'create'","title":"record.operation"},{"location":"dynamodb/configuration-details/#recordsource_arn","text":"1 2 3 4 print ( record . source_arn ); # output 'arn:aws:dynamodb:us-east-1:771875143460:table/test-example/stream/2019-10-04T23:18:26.340'","title":"record.source_arn"},{"location":"dynamodb/configuration-details/#recordversion","text":"1 2 3 4 print ( record . version ); # output '1.1'","title":"record.version"},{"location":"dynamodb/configuration-details/#recordstream_type","text":"1 2 3 4 print ( record . stream_type ); # output 'NEW_AND_OLD_IMAGES'","title":"record.stream_type"},{"location":"dynamodb/configuration-details/#recordsize","text":"1 2 3 4 print ( record . size ); # output 1124","title":"record.size"},{"location":"dynamodb/configuration-details/#recordcreated","text":"1 2 3 4 print ( record . created ); # output 1538695200.0 #unix timestamp","title":"record.created"},{"location":"dynamodb/configuration-details/#recordidentity","text":"1 2 3 4 5 6 7 print ( record . identity ); # output { 'type' : 'Service' , 'principalId' : 'dynamodb.amazonaws.com' }","title":"record.identity"},{"location":"dynamodb/configuration-details/#recordexpired","text":"1 2 3 4 print ( record . expired ); # output False","title":"record.expired"},{"location":"dynamodb/quickstart/","text":"DynamoDB Quickstart \u00b6 Event handler for Amazon DynamoDB Stream Events. Features \u00b6 Automatically convert DynamoDB JSON dict to standard JSON dict Only run on certain DynamoDB operations; i.e. when items are created, updated or deleted Able to validate DynamoDB record against a JSON Schema Assign Data Classes to records instead of getting DynamoDB JSON dicts Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. dynamodb.py serverless.yml 1 2 3 4 5 6 7 8 from acai_aws.dynamodb.requirements import requirements @requirements ( operations = [ 'created' , 'deleted' ] ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 10 functions : ddb-handler : handler : service/handlers/dynamodb.handle memorySize : 512 timeout : 30 events : - stream : type : dynamodb arn : Fn::GetAtt : [ SomeDDBTable , StreamArn ]","title":"Quickstart"},{"location":"dynamodb/quickstart/#dynamodb-quickstart","text":"Event handler for Amazon DynamoDB Stream Events.","title":"DynamoDB Quickstart"},{"location":"dynamodb/quickstart/#features","text":"Automatically convert DynamoDB JSON dict to standard JSON dict Only run on certain DynamoDB operations; i.e. when items are created, updated or deleted Able to validate DynamoDB record against a JSON Schema Assign Data Classes to records instead of getting DynamoDB JSON dicts","title":"Features"},{"location":"dynamodb/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"dynamodb/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. dynamodb.py serverless.yml 1 2 3 4 5 6 7 8 from acai_aws.dynamodb.requirements import requirements @requirements ( operations = [ 'created' , 'deleted' ] ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 10 functions : ddb-handler : handler : service/handlers/dynamodb.handle memorySize : 512 timeout : 30 events : - stream : type : dynamodb arn : Fn::GetAtt : [ SomeDDBTable , StreamArn ]","title":"Minimal Setup"},{"location":"firehose/configuration-details/","text":"Firehose Configurations Details \u00b6 The firehose event will automatically handle many common things done when eventing off a firehose stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : firehose-handler : handler : service/handlers/firehose.handle memorySize : 512 timeout : 30 events : - stream : type : firehose arn : Fn::GetAtt : [ MyfirehoseStream , Arn ] Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from acai_aws.firehose.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-firehose-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record ) Firehose Record Properties \u00b6 property type description record_id str record id of the stream epoc_time_stamp int epoc time stamp of the stream shard_id str shard id arn of the stream subsequence_number str subsequence number arn of the stream partition_key str partition key time_stamp str time stamp sequence_number str sequence number data any can be anything, automaticallyed b64 decoded objects body any can be anything, automaticallyed b64 decoded objects record.record_id \u00b6 1 2 3 4 print ( record . record_id ); # output 'record1' record.epoc_time_stamp \u00b6 1 2 3 4 print ( record . epoc_time_stamp ); # output 1510772160000 record.shard_id \u00b6 1 2 3 4 print ( record . shard_id ); # output 'shardId-000000000000' record.subsequence_number \u00b6 1 2 3 4 print ( record . subsequence_number ); # output '' record.partition_key \u00b6 1 2 3 4 print ( record . partition_key ); # output '1' record.time_stamp \u00b6 1 2 3 4 print ( record . time_stamp ); # output 1545084650.987 record.sequence_number \u00b6 1 2 3 4 print ( record . sequence_number ); # output 49590338271490256608559692538361571095921575989136588898 record.data \u00b6 1 2 3 4 5 6 print ( record . data ); # output { 'new_data' : '123456789' } record.body \u00b6 1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' }","title":"Configurations Details"},{"location":"firehose/configuration-details/#firehose-configurations-details","text":"The firehose event will automatically handle many common things done when eventing off a firehose stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"Firehose Configurations Details"},{"location":"firehose/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : firehose-handler : handler : service/handlers/firehose.handle memorySize : 512 timeout : 30 events : - stream : type : firehose arn : Fn::GetAtt : [ MyfirehoseStream , Arn ]","title":"Lambda Configuration"},{"location":"firehose/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from acai_aws.firehose.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-firehose-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record )","title":"Requirements Configuration Options"},{"location":"firehose/configuration-details/#firehose-record-properties","text":"property type description record_id str record id of the stream epoc_time_stamp int epoc time stamp of the stream shard_id str shard id arn of the stream subsequence_number str subsequence number arn of the stream partition_key str partition key time_stamp str time stamp sequence_number str sequence number data any can be anything, automaticallyed b64 decoded objects body any can be anything, automaticallyed b64 decoded objects","title":"Firehose Record Properties"},{"location":"firehose/configuration-details/#recordrecord_id","text":"1 2 3 4 print ( record . record_id ); # output 'record1'","title":"record.record_id"},{"location":"firehose/configuration-details/#recordepoc_time_stamp","text":"1 2 3 4 print ( record . epoc_time_stamp ); # output 1510772160000","title":"record.epoc_time_stamp"},{"location":"firehose/configuration-details/#recordshard_id","text":"1 2 3 4 print ( record . shard_id ); # output 'shardId-000000000000'","title":"record.shard_id"},{"location":"firehose/configuration-details/#recordsubsequence_number","text":"1 2 3 4 print ( record . subsequence_number ); # output ''","title":"record.subsequence_number"},{"location":"firehose/configuration-details/#recordpartition_key","text":"1 2 3 4 print ( record . partition_key ); # output '1'","title":"record.partition_key"},{"location":"firehose/configuration-details/#recordtime_stamp","text":"1 2 3 4 print ( record . time_stamp ); # output 1545084650.987","title":"record.time_stamp"},{"location":"firehose/configuration-details/#recordsequence_number","text":"1 2 3 4 print ( record . sequence_number ); # output 49590338271490256608559692538361571095921575989136588898","title":"record.sequence_number"},{"location":"firehose/configuration-details/#recorddata","text":"1 2 3 4 5 6 print ( record . data ); # output { 'new_data' : '123456789' }","title":"record.data"},{"location":"firehose/configuration-details/#recordbody","text":"1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' }","title":"record.body"},{"location":"firehose/quickstart/","text":"Firehose Quickstart \u00b6 Event handler for Amazon firehose Stream Events. Features \u00b6 Automatically decode firehose base 64 string into dict (or whatever the decode thing is) Able to validate firehose record against a JSON Schema Assign Data Classes to records instead of getting firehose JSON dicts Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. firehose.py serverless.yml 1 2 3 4 5 6 from acai_aws.firehose.requirements import requirements @requirements () def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 10 functions : firehose-handler : handler : service/handlers/firehose.handle memorySize : 512 timeout : 30 events : - stream : type : firehose arn : Fn::GetAtt : [ MyFirehoseStream , Arn ]","title":"Quickstart"},{"location":"firehose/quickstart/#firehose-quickstart","text":"Event handler for Amazon firehose Stream Events.","title":"Firehose Quickstart"},{"location":"firehose/quickstart/#features","text":"Automatically decode firehose base 64 string into dict (or whatever the decode thing is) Able to validate firehose record against a JSON Schema Assign Data Classes to records instead of getting firehose JSON dicts","title":"Features"},{"location":"firehose/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"firehose/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. firehose.py serverless.yml 1 2 3 4 5 6 from acai_aws.firehose.requirements import requirements @requirements () def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 10 functions : firehose-handler : handler : service/handlers/firehose.handle memorySize : 512 timeout : 30 events : - stream : type : firehose arn : Fn::GetAtt : [ MyFirehoseStream , Arn ]","title":"Minimal Setup"},{"location":"generic/configuration-details/","text":"Generic (AWS Console or CLI) Configurations Details \u00b6 The generic event will automatically handle many common things done when eventing off a generic event invoked manually or programmatically. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 functions : generic-handler : handler : service/handlers/generic.handle memorySize : 512 timeout : 30 Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from acai_aws.firehose.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): print ( event . body ) # example after function def alert_something ( event , result , requirements ): if 'something' in result and 'alert' in requirements : print ( event ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-generic-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): print ( event . body ) Generic Event Properties \u00b6 property type description body any can be anything, automaticallyed b64 decoded objects event.body \u00b6 1 2 3 4 5 6 print ( event . body ); # output { 'new_data' : '123456789' }","title":"Configurations Details"},{"location":"generic/configuration-details/#generic-aws-console-or-cli-configurations-details","text":"The generic event will automatically handle many common things done when eventing off a generic event invoked manually or programmatically. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"Generic (AWS Console or CLI) Configurations Details"},{"location":"generic/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 functions : generic-handler : handler : service/handlers/generic.handle memorySize : 512 timeout : 30","title":"Lambda Configuration"},{"location":"generic/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from acai_aws.firehose.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): print ( event . body ) # example after function def alert_something ( event , result , requirements ): if 'something' in result and 'alert' in requirements : print ( event ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-generic-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): print ( event . body )","title":"Requirements Configuration Options"},{"location":"generic/configuration-details/#generic-event-properties","text":"property type description body any can be anything, automaticallyed b64 decoded objects","title":"Generic Event Properties"},{"location":"generic/configuration-details/#eventbody","text":"1 2 3 4 5 6 print ( event . body ); # output { 'new_data' : '123456789' }","title":"event.body"},{"location":"generic/quickstart/","text":"Generic (AWS Console or CLI) Quickstart \u00b6 Event handler for Amazon generic (AWS Console or CLI) Events. Features \u00b6 Automatically convert body into dict Able to validate body of record against a JSON Schema Assign Data Classes to records instead of getting firehose JSON dicts Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. generic.py serverless.yml 1 2 3 4 5 from acai_aws.generic.requirements import requirements @requirements () def handle ( event ): print ( event . body ) 1 2 3 4 5 functions : generic-handler : handler : service/handlers/generic.handle memorySize : 512 timeout : 30","title":"Quickstart"},{"location":"generic/quickstart/#generic-aws-console-or-cli-quickstart","text":"Event handler for Amazon generic (AWS Console or CLI) Events.","title":"Generic (AWS Console or CLI) Quickstart"},{"location":"generic/quickstart/#features","text":"Automatically convert body into dict Able to validate body of record against a JSON Schema Assign Data Classes to records instead of getting firehose JSON dicts","title":"Features"},{"location":"generic/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"generic/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. generic.py serverless.yml 1 2 3 4 5 from acai_aws.generic.requirements import requirements @requirements () def handle ( event ): print ( event . body ) 1 2 3 4 5 functions : generic-handler : handler : service/handlers/generic.handle memorySize : 512 timeout : 30","title":"Minimal Setup"},{"location":"kinesis/configuration-details/","text":"Kinesis Configurations Details \u00b6 The kinesis event will automatically handle many common things done when eventing off a kinesis stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : kinesis-handler : handler : service/handlers/kinesis.handle memorySize : 512 timeout : 30 events : - stream : type : kinesis arn : Fn::GetAtt : [ MyKinesisStream , Arn ] Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from acai_aws.kinesis.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-kinesis-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record ) Kinesis Record Properties \u00b6 property type description id str id of the stream name str name of the stream source_arn str source arn of the stream region str region arn of the stream version str version arn of the stream invoke_identity_arn str arn of the indentity schema_version str schema version partition_key str partition key time_stamp str time stamp sequence_number str sequence number data any can be anything, automaticallyed b64 decoded objects body any can be anything, automaticallyed b64 decoded objects record.id \u00b6 1 2 3 4 print ( record . id ); # output 'shardId-000000000006:49590338271490256608559692538361571095921575989136588898' record.name \u00b6 1 2 3 4 print ( record . name ); # output 'aws:kinesis:record' record.source_arn \u00b6 1 2 3 4 print ( record . source_arn ); # output 'arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream' record.region \u00b6 1 2 3 4 print ( record . region ); # output 'us-east-2' record.version \u00b6 1 2 3 4 print ( record . version ); # output '1.0' record.invoke_identity_arn \u00b6 1 2 3 4 print ( record . invoke_identity_arn ); # output 'arn:aws:iam::123456789012:role/lambda-role' record.schema_version \u00b6 1 2 3 4 print ( record . schema_version ); # output '1.0' record.partition_key \u00b6 1 2 3 4 print ( record . partition_key ); # output '1' record.time_stamp \u00b6 1 2 3 4 print ( record . time_stamp ); # output 1545084650.987 record.sequence_number \u00b6 1 2 3 4 print ( record . sequence_number ); # output 49590338271490256608559692538361571095921575989136588898 record.data \u00b6 1 2 3 4 5 6 print ( record . data ); # output { 'new_data' : '123456789' } record.body \u00b6 1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' }","title":"Configurations Details"},{"location":"kinesis/configuration-details/#kinesis-configurations-details","text":"The kinesis event will automatically handle many common things done when eventing off a kinesis stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"Kinesis Configurations Details"},{"location":"kinesis/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : kinesis-handler : handler : service/handlers/kinesis.handle memorySize : 512 timeout : 30 events : - stream : type : kinesis arn : Fn::GetAtt : [ MyKinesisStream , Arn ]","title":"Lambda Configuration"},{"location":"kinesis/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from acai_aws.kinesis.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-kinesis-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record )","title":"Requirements Configuration Options"},{"location":"kinesis/configuration-details/#kinesis-record-properties","text":"property type description id str id of the stream name str name of the stream source_arn str source arn of the stream region str region arn of the stream version str version arn of the stream invoke_identity_arn str arn of the indentity schema_version str schema version partition_key str partition key time_stamp str time stamp sequence_number str sequence number data any can be anything, automaticallyed b64 decoded objects body any can be anything, automaticallyed b64 decoded objects","title":"Kinesis Record Properties"},{"location":"kinesis/configuration-details/#recordid","text":"1 2 3 4 print ( record . id ); # output 'shardId-000000000006:49590338271490256608559692538361571095921575989136588898'","title":"record.id"},{"location":"kinesis/configuration-details/#recordname","text":"1 2 3 4 print ( record . name ); # output 'aws:kinesis:record'","title":"record.name"},{"location":"kinesis/configuration-details/#recordsource_arn","text":"1 2 3 4 print ( record . source_arn ); # output 'arn:aws:kinesis:us-east-2:123456789012:stream/lambda-stream'","title":"record.source_arn"},{"location":"kinesis/configuration-details/#recordregion","text":"1 2 3 4 print ( record . region ); # output 'us-east-2'","title":"record.region"},{"location":"kinesis/configuration-details/#recordversion","text":"1 2 3 4 print ( record . version ); # output '1.0'","title":"record.version"},{"location":"kinesis/configuration-details/#recordinvoke_identity_arn","text":"1 2 3 4 print ( record . invoke_identity_arn ); # output 'arn:aws:iam::123456789012:role/lambda-role'","title":"record.invoke_identity_arn"},{"location":"kinesis/configuration-details/#recordschema_version","text":"1 2 3 4 print ( record . schema_version ); # output '1.0'","title":"record.schema_version"},{"location":"kinesis/configuration-details/#recordpartition_key","text":"1 2 3 4 print ( record . partition_key ); # output '1'","title":"record.partition_key"},{"location":"kinesis/configuration-details/#recordtime_stamp","text":"1 2 3 4 print ( record . time_stamp ); # output 1545084650.987","title":"record.time_stamp"},{"location":"kinesis/configuration-details/#recordsequence_number","text":"1 2 3 4 print ( record . sequence_number ); # output 49590338271490256608559692538361571095921575989136588898","title":"record.sequence_number"},{"location":"kinesis/configuration-details/#recorddata","text":"1 2 3 4 5 6 print ( record . data ); # output { 'new_data' : '123456789' }","title":"record.data"},{"location":"kinesis/configuration-details/#recordbody","text":"1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' }","title":"record.body"},{"location":"kinesis/quickstart/","text":"Kinesis Quickstart \u00b6 Event handler for Amazon Kinesis Stream Events. Features \u00b6 Automatically decode kinesis base 64 string into dict (or whatever the decode thing is) Able to validate Kinesis record against a JSON Schema Assign Data Classes to records instead of getting Kinesis JSON dicts Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. kinesis.py serverless.yml 1 2 3 4 5 6 from acai_aws.kinesis.requirements import requirements @requirements () def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 10 functions : kinesis-handler : handler : service/handlers/Kinesis.handle memorySize : 512 timeout : 30 events : - stream : type : Kinesis arn : Fn::GetAtt : [ MyKinesisStream , Arn ]","title":"Quickstart"},{"location":"kinesis/quickstart/#kinesis-quickstart","text":"Event handler for Amazon Kinesis Stream Events.","title":"Kinesis Quickstart"},{"location":"kinesis/quickstart/#features","text":"Automatically decode kinesis base 64 string into dict (or whatever the decode thing is) Able to validate Kinesis record against a JSON Schema Assign Data Classes to records instead of getting Kinesis JSON dicts","title":"Features"},{"location":"kinesis/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"kinesis/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. kinesis.py serverless.yml 1 2 3 4 5 6 from acai_aws.kinesis.requirements import requirements @requirements () def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 10 functions : kinesis-handler : handler : service/handlers/Kinesis.handle memorySize : 512 timeout : 30 events : - stream : type : Kinesis arn : Fn::GetAtt : [ MyKinesisStream , Arn ]","title":"Minimal Setup"},{"location":"mq/configuration-details/","text":"MQ Configurations Details \u00b6 The mq event will automatically handle many common things done when eventing off a generic event invoked manually or programmatically. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : generic-handler : handler : service/handlers/mq.handle memorySize : 512 timeout : 30 events : - rabbitmq : arn : arn:aws:mq:us-east-1:0000:broker:ExampleMQBroker:b-xxx-xxx queue : queue-name basicAuthArn : arn:aws:secretsmanager:us-east-1:01234567890:secret:MySecret Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from acai_aws.firehose.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): print ( event . body ) # example after function def alert_something ( event , result , requirements ): if 'something' in result and 'alert' in requirements : print ( event ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-mq-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record . body ) MQ Event Properties \u00b6 property type description message_id str id of the message message_type str type of the message delivery_mode int mode of delivery reply_to str (nullable) reply to string record_type str (nullable) type of record expiration str expiration of message priority int priority of message correlation_id str correlation id of message redelivered bool whether the message has been redelivered destination dict destination of the message properties dict properties of the message time_stamp int time_stamp of the message in_time int in time of the message out_time int out time of the message body any body of the message data any data of the message record.message_id \u00b6 1 2 3 4 print ( record . message_id ); # output 'ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-west-2.amazonaws.com-37557-1234520418293-4:1:1:1:1' record.message_type \u00b6 1 2 3 4 print ( record . message_type ); # output 'jms/text-message' record.delivery_mode \u00b6 1 2 3 4 print ( record . delivery_mode ); # output 1 record.reply_to \u00b6 1 2 3 4 print ( record . reply_to ); # output None record.record_type \u00b6 1 2 3 4 print ( record . record_type ); # output None record.expiration \u00b6 1 2 3 4 print ( record . expiration ); # output '60000' record.priority \u00b6 1 2 3 4 print ( record . priority ); # output 1 record.correlation_id \u00b6 1 2 3 4 print ( record . correlation_id ); # output 'myJMSCoID' record.redelivered \u00b6 1 2 3 4 print ( record . redelivered ); # output False record.destination \u00b6 1 2 3 4 5 6 print ( record . destination ); # output { 'physicalName' : 'testQueue' } record.properties \u00b6 1 2 3 4 5 6 7 8 print ( record . properties ); # output { 'index' : '1' , 'doAlarm' : 'false' , 'myCustomProperty' : 'value' } record.time_stamp \u00b6 1 2 3 4 print ( record . time_stamp ); # output 1598827811958 record.in_time \u00b6 1 2 3 4 print ( record . in_time ); # output 1598827811958 record.out_time \u00b6 1 2 3 4 print ( record . out_time ); # output 1598827811959 record.body \u00b6 1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' } record.data \u00b6 1 2 3 4 5 6 print ( record . data ); # output { 'new_data' : '123456789' }","title":"Configurations Details"},{"location":"mq/configuration-details/#mq-configurations-details","text":"The mq event will automatically handle many common things done when eventing off a generic event invoked manually or programmatically. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"MQ Configurations Details"},{"location":"mq/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 6 7 8 9 10 functions : generic-handler : handler : service/handlers/mq.handle memorySize : 512 timeout : 30 events : - rabbitmq : arn : arn:aws:mq:us-east-1:0000:broker:ExampleMQBroker:b-xxx-xxx queue : queue-name basicAuthArn : arn:aws:secretsmanager:us-east-1:01234567890:secret:MySecret","title":"Lambda Configuration"},{"location":"mq/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from acai_aws.firehose.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): print ( event . body ) # example after function def alert_something ( event , result , requirements ): if 'something' in result and 'alert' in requirements : print ( event ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-mq-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record . body )","title":"Requirements Configuration Options"},{"location":"mq/configuration-details/#mq-event-properties","text":"property type description message_id str id of the message message_type str type of the message delivery_mode int mode of delivery reply_to str (nullable) reply to string record_type str (nullable) type of record expiration str expiration of message priority int priority of message correlation_id str correlation id of message redelivered bool whether the message has been redelivered destination dict destination of the message properties dict properties of the message time_stamp int time_stamp of the message in_time int in time of the message out_time int out time of the message body any body of the message data any data of the message","title":"MQ Event Properties"},{"location":"mq/configuration-details/#recordmessage_id","text":"1 2 3 4 print ( record . message_id ); # output 'ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-west-2.amazonaws.com-37557-1234520418293-4:1:1:1:1'","title":"record.message_id"},{"location":"mq/configuration-details/#recordmessage_type","text":"1 2 3 4 print ( record . message_type ); # output 'jms/text-message'","title":"record.message_type"},{"location":"mq/configuration-details/#recorddelivery_mode","text":"1 2 3 4 print ( record . delivery_mode ); # output 1","title":"record.delivery_mode"},{"location":"mq/configuration-details/#recordreply_to","text":"1 2 3 4 print ( record . reply_to ); # output None","title":"record.reply_to"},{"location":"mq/configuration-details/#recordrecord_type","text":"1 2 3 4 print ( record . record_type ); # output None","title":"record.record_type"},{"location":"mq/configuration-details/#recordexpiration","text":"1 2 3 4 print ( record . expiration ); # output '60000'","title":"record.expiration"},{"location":"mq/configuration-details/#recordpriority","text":"1 2 3 4 print ( record . priority ); # output 1","title":"record.priority"},{"location":"mq/configuration-details/#recordcorrelation_id","text":"1 2 3 4 print ( record . correlation_id ); # output 'myJMSCoID'","title":"record.correlation_id"},{"location":"mq/configuration-details/#recordredelivered","text":"1 2 3 4 print ( record . redelivered ); # output False","title":"record.redelivered"},{"location":"mq/configuration-details/#recorddestination","text":"1 2 3 4 5 6 print ( record . destination ); # output { 'physicalName' : 'testQueue' }","title":"record.destination"},{"location":"mq/configuration-details/#recordproperties","text":"1 2 3 4 5 6 7 8 print ( record . properties ); # output { 'index' : '1' , 'doAlarm' : 'false' , 'myCustomProperty' : 'value' }","title":"record.properties"},{"location":"mq/configuration-details/#recordtime_stamp","text":"1 2 3 4 print ( record . time_stamp ); # output 1598827811958","title":"record.time_stamp"},{"location":"mq/configuration-details/#recordin_time","text":"1 2 3 4 print ( record . in_time ); # output 1598827811958","title":"record.in_time"},{"location":"mq/configuration-details/#recordout_time","text":"1 2 3 4 print ( record . out_time ); # output 1598827811959","title":"record.out_time"},{"location":"mq/configuration-details/#recordbody","text":"1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' }","title":"record.body"},{"location":"mq/configuration-details/#recorddata","text":"1 2 3 4 5 6 print ( record . data ); # output { 'new_data' : '123456789' }","title":"record.data"},{"location":"mq/quickstart/","text":"MQ Quickstart \u00b6 Event handler for Amazon MQ Events. Features \u00b6 Automatically decode base 64 string into dict (or whatever the decode thing is) Able to validate body of record against a JSON Schema Assign Data Classes to records instead of getting firehose JSON dicts Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. generic.py serverless.yml 1 2 3 4 5 6 from acai_aws.mq.requirements import requirements @requirements () def handle ( event ): for record in event . records : print ( record . body ) 1 2 3 4 5 6 7 8 9 10 functions : mq-handler : handler : service/handlers/mq.handle memorySize : 512 timeout : 30 events : - rabbitmq : arn : arn:aws:mq:us-east-1:0000:broker:ExampleMQBroker:b-xxx-xxx queue : queue-name basicAuthArn : arn:aws:secretsmanager:us-east-1:01234567890:secret:MySecret","title":"Quickstart"},{"location":"mq/quickstart/#mq-quickstart","text":"Event handler for Amazon MQ Events.","title":"MQ Quickstart"},{"location":"mq/quickstart/#features","text":"Automatically decode base 64 string into dict (or whatever the decode thing is) Able to validate body of record against a JSON Schema Assign Data Classes to records instead of getting firehose JSON dicts","title":"Features"},{"location":"mq/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"mq/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. generic.py serverless.yml 1 2 3 4 5 6 from acai_aws.mq.requirements import requirements @requirements () def handle ( event ): for record in event . records : print ( record . body ) 1 2 3 4 5 6 7 8 9 10 functions : mq-handler : handler : service/handlers/mq.handle memorySize : 512 timeout : 30 events : - rabbitmq : arn : arn:aws:mq:us-east-1:0000:broker:ExampleMQBroker:b-xxx-xxx queue : queue-name basicAuthArn : arn:aws:secretsmanager:us-east-1:01234567890:secret:MySecret","title":"Minimal Setup"},{"location":"msk/configuration-details/","text":"MSK Configurations Details \u00b6 The msk event will automatically handle many common things done when eventing off a generic event invoked manually or programmatically. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 6 7 8 9 functions : msk-handler : handler : service/handlers/msk.handle memorySize : 512 timeout : 30 events : - msk : arn : arn:aws:kafka:region:XXXXXX:cluster/MyCluster/xxxx-xxxxx-xxxx topic : mytopic Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from acai_aws.firehose.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): print ( event . body ) # example after function def alert_something ( event , result , requirements ): if 'something' in result and 'alert' in requirements : print ( event ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-msk-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record . body ) MQ Event Properties \u00b6 property type description message_id str id of the message message_type str type of the message delivery_mode int mode of delivery reply_to str (nullable) reply to string record_type str (nullable) type of record expiration str expiration of message priority int priority of message correlation_id str correlation id of message redelivered bool whether the message has been redelivered destination dict destination of the message properties dict properties of the message time_stamp int time_stamp of the message in_time int in time of the message out_time int out time of the message body any body of the message data any data of the message record.message_id \u00b6 1 2 3 4 print ( record . message_id ); # output 'ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-west-2.amazonaws.com-37557-1234520418293-4:1:1:1:1' record.message_type \u00b6 1 2 3 4 print ( record . message_type ); # output 'jms/text-message' record.delivery_mode \u00b6 1 2 3 4 print ( record . delivery_mode ); # output 1 record.reply_to \u00b6 1 2 3 4 print ( record . reply_to ); # output None record.record_type \u00b6 1 2 3 4 print ( record . record_type ); # output None record.expiration \u00b6 1 2 3 4 print ( record . expiration ); # output '60000' record.priority \u00b6 1 2 3 4 print ( record . priority ); # output 1 record.correlation_id \u00b6 1 2 3 4 print ( record . correlation_id ); # output 'myJMSCoID' record.redelivered \u00b6 1 2 3 4 print ( record . redelivered ); # output False record.destination \u00b6 1 2 3 4 5 6 print ( record . destination ); # output { 'physicalName' : 'testQueue' } record.properties \u00b6 1 2 3 4 5 6 7 8 print ( record . properties ); # output { 'index' : '1' , 'doAlarm' : 'false' , 'myCustomProperty' : 'value' } record.time_stamp \u00b6 1 2 3 4 print ( record . time_stamp ); # output 1598827811958 record.in_time \u00b6 1 2 3 4 print ( record . in_time ); # output 1598827811958 record.out_time \u00b6 1 2 3 4 print ( record . out_time ); # output 1598827811959 record.body \u00b6 1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' } record.data \u00b6 1 2 3 4 5 6 print ( record . data ); # output { 'new_data' : '123456789' }","title":"Configurations Details"},{"location":"msk/configuration-details/#msk-configurations-details","text":"The msk event will automatically handle many common things done when eventing off a generic event invoked manually or programmatically. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"MSK Configurations Details"},{"location":"msk/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 6 7 8 9 functions : msk-handler : handler : service/handlers/msk.handle memorySize : 512 timeout : 30 events : - msk : arn : arn:aws:kafka:region:XXXXXX:cluster/MyCluster/xxxx-xxxxx-xxxx topic : mytopic","title":"Lambda Configuration"},{"location":"msk/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from acai_aws.firehose.requirements import requirements # example data class class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): print ( event . body ) # example after function def alert_something ( event , result , requirements ): if 'something' in result and 'alert' in requirements : print ( event ) @requirements ( before = log_something , data_class = SomeClass , raise_operation_error = True , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-msk-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record . body )","title":"Requirements Configuration Options"},{"location":"msk/configuration-details/#mq-event-properties","text":"property type description message_id str id of the message message_type str type of the message delivery_mode int mode of delivery reply_to str (nullable) reply to string record_type str (nullable) type of record expiration str expiration of message priority int priority of message correlation_id str correlation id of message redelivered bool whether the message has been redelivered destination dict destination of the message properties dict properties of the message time_stamp int time_stamp of the message in_time int in time of the message out_time int out time of the message body any body of the message data any data of the message","title":"MQ Event Properties"},{"location":"msk/configuration-details/#recordmessage_id","text":"1 2 3 4 print ( record . message_id ); # output 'ID:b-9bcfa592-423a-4942-879d-eb284b418fc8-1.mq.us-west-2.amazonaws.com-37557-1234520418293-4:1:1:1:1'","title":"record.message_id"},{"location":"msk/configuration-details/#recordmessage_type","text":"1 2 3 4 print ( record . message_type ); # output 'jms/text-message'","title":"record.message_type"},{"location":"msk/configuration-details/#recorddelivery_mode","text":"1 2 3 4 print ( record . delivery_mode ); # output 1","title":"record.delivery_mode"},{"location":"msk/configuration-details/#recordreply_to","text":"1 2 3 4 print ( record . reply_to ); # output None","title":"record.reply_to"},{"location":"msk/configuration-details/#recordrecord_type","text":"1 2 3 4 print ( record . record_type ); # output None","title":"record.record_type"},{"location":"msk/configuration-details/#recordexpiration","text":"1 2 3 4 print ( record . expiration ); # output '60000'","title":"record.expiration"},{"location":"msk/configuration-details/#recordpriority","text":"1 2 3 4 print ( record . priority ); # output 1","title":"record.priority"},{"location":"msk/configuration-details/#recordcorrelation_id","text":"1 2 3 4 print ( record . correlation_id ); # output 'myJMSCoID'","title":"record.correlation_id"},{"location":"msk/configuration-details/#recordredelivered","text":"1 2 3 4 print ( record . redelivered ); # output False","title":"record.redelivered"},{"location":"msk/configuration-details/#recorddestination","text":"1 2 3 4 5 6 print ( record . destination ); # output { 'physicalName' : 'testQueue' }","title":"record.destination"},{"location":"msk/configuration-details/#recordproperties","text":"1 2 3 4 5 6 7 8 print ( record . properties ); # output { 'index' : '1' , 'doAlarm' : 'false' , 'myCustomProperty' : 'value' }","title":"record.properties"},{"location":"msk/configuration-details/#recordtime_stamp","text":"1 2 3 4 print ( record . time_stamp ); # output 1598827811958","title":"record.time_stamp"},{"location":"msk/configuration-details/#recordin_time","text":"1 2 3 4 print ( record . in_time ); # output 1598827811958","title":"record.in_time"},{"location":"msk/configuration-details/#recordout_time","text":"1 2 3 4 print ( record . out_time ); # output 1598827811959","title":"record.out_time"},{"location":"msk/configuration-details/#recordbody","text":"1 2 3 4 5 6 print ( record . body ); # output { 'new_data' : '123456789' }","title":"record.body"},{"location":"msk/configuration-details/#recorddata","text":"1 2 3 4 5 6 print ( record . data ); # output { 'new_data' : '123456789' }","title":"record.data"},{"location":"msk/quickstart/","text":"MSK Quickstart \u00b6 Event handler for Amazon MSK Events. Features \u00b6 Automatically decodes base 64 string into dict (or whatever the decode thing is) Able to validate body of record against a JSON Schema Assign Data Classes to records instead of getting firehose JSON dicts Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. generic.py serverless.yml 1 2 3 4 5 6 from acai_aws.msk.requirements import requirements @requirements () def handle ( event ): for record in event . records : print ( record . body ) 1 2 3 4 5 6 7 8 9 functions : msk-handler : handler : service/handlers/msk.handle memorySize : 512 timeout : 30 events : - msk : arn : arn:aws:kafka:region:XXXXXX:cluster/MyCluster/xxxx-xxxxx-xxxx topic : mytopic","title":"Quickstart"},{"location":"msk/quickstart/#msk-quickstart","text":"Event handler for Amazon MSK Events.","title":"MSK Quickstart"},{"location":"msk/quickstart/#features","text":"Automatically decodes base 64 string into dict (or whatever the decode thing is) Able to validate body of record against a JSON Schema Assign Data Classes to records instead of getting firehose JSON dicts","title":"Features"},{"location":"msk/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"msk/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. generic.py serverless.yml 1 2 3 4 5 6 from acai_aws.msk.requirements import requirements @requirements () def handle ( event ): for record in event . records : print ( record . body ) 1 2 3 4 5 6 7 8 9 functions : msk-handler : handler : service/handlers/msk.handle memorySize : 512 timeout : 30 events : - msk : arn : arn:aws:kafka:region:XXXXXX:cluster/MyCluster/xxxx-xxxxx-xxxx topic : mytopic","title":"Minimal Setup"},{"location":"s3/configuration-details/","text":"S3 Configurations Details \u00b6 The S3 event will automatically handle many common things done when eventing off a S3 event. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 6 7 8 9 functions : s3-handler : handler : service/handlers/s3.handle memorySize : 512 timeout : 900 events : - s3 : bucket : uploadsBucket event : s3:ObjectCreated:* Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object data_type enum (str) no ['json', 'csv'] will convert data to a dict based on type; requires get_object=True get_object bool no False will download object from s3 bucket and hold in memory operations list no ['created', 'updated', 'deleted'] will only run if record was created from the listed operation raise_operation_error bool no False will raise exception if operation of record is not from listed operations raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from acai_aws.s3.requirements import requirements # example data class (requires, get_object=True and a data_type) class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , get_object = True , data_type = 'json' , data_class = SomeClass , raise_body_error = True , # requires, get_object=True and a data_type schema = 'service/openapi.yml' , required_body = 'v1-s3-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record ) S3 Record Properties \u00b6 property type description name str the name of the event source str the source of the event version str the version of the event time str the time of the event region str the region of the event request dict the request parameters of the event response dict the response parameters of the event configuration_id str the configuration id object dict the object dict of the event bucket dict the bucket dict of the event bucket dict the bucket dict of the event bucket_arn str the arn of the bucket bucket_owner str the owner of the bucket key str the key owner of the object schema_version str the schema version user_identity str the user identity operation enum (str) enum of created , deleted , updated body dict,bytes the body of file in the s3 bucket record.name \u00b6 1 2 3 4 print ( record . name ) # output 'ObjectRemoved: Put' record.source \u00b6 1 2 3 4 print ( record . source ) # output 'aws:s3' record.version \u00b6 1 2 3 4 print ( record . version ) # output '2.0' record.time \u00b6 1 2 3 4 print ( record . time ) # output '2018-09-20T21: 10: 13.821Z' record.region \u00b6 1 2 3 4 print ( record . region ) # output 'us-east-1' record.request \u00b6 1 2 3 4 5 6 print ( record . request ) # output { 'sourceIPAddress' : '172.20.133.36' } record.response \u00b6 1 2 3 4 5 6 7 print ( record . response ) # output { 'x-amz-request-id' : '6B859DD0CE613FAE' , 'x-amz-id-2' : 'EXLMfc9aiXZFzNwLKXpw35iaVvl/DkEA6GtbuxjfmuLN3kLPL/aGoa7NMSwpl3m7ICAtNbjJX4w=' } record.configuration_id \u00b6 1 2 3 4 5 6 7 print ( record . configuration_id ) # output { 'x-amz-request-id' : '6B859DD0CE613FAE' , 'x-amz-id-2' : 'EXLMfc9aiXZFzNwLKXpw35iaVvl/DkEA6GtbuxjfmuLN3kLPL/aGoa7NMSwpl3m7ICAtNbjJX4w=' } record.object \u00b6 1 2 3 4 5 6 7 8 9 print ( record . object ) # output { 'key' : 'user-1-prefs.json' , 'size' : 17545 , 'eTag' : 'b79ac2ef68c08fa9ac6013d53038a26c' , 'sequencer' : '005BA40CB5BD42013A' } record.bucket \u00b6 1 2 3 4 5 6 7 8 9 10 print ( record . bucket ) # output { 'name' : 'user-preferences' , 'ownerIdentity' : { 'principalId' : 'A32KFL0DQ3MH8X' }, 'arn' : 'arn:aws:s3:::user-preferences' } record.bucket_arn \u00b6 1 2 3 4 print ( record . bucket_arn ) # output 'arn:aws:s3:::user-preferences' record.bucket_owner \u00b6 1 2 3 4 print ( record . bucket_owner ) # output 'A32KFL0DQ3MH8X' record.key \u00b6 1 2 3 4 print ( record . key ) # output 'user-1-prefs.json' record.schema_version \u00b6 1 2 3 4 print ( record . schema_version ) # output '1.0' record.user_identity \u00b6 1 2 3 4 print ( record . user_identity ) # output 'AWS: AROAI7Z5ZQEQ3UETKKYGQ: deploy-workers-poc-put-v1-photo' record.operation \u00b6 1 2 3 4 print ( record . operation ) # output 'created' record.body \u00b6 1 2 3 4 5 6 print ( record . body ) # output { 'possible' : 'output' }","title":"Configurations Details"},{"location":"s3/configuration-details/#s3-configurations-details","text":"The S3 event will automatically handle many common things done when eventing off a S3 event. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"S3 Configurations Details"},{"location":"s3/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 6 7 8 9 functions : s3-handler : handler : service/handlers/s3.handle memorySize : 512 timeout : 900 events : - s3 : bucket : uploadsBucket event : s3:ObjectCreated:*","title":"Lambda Configuration"},{"location":"s3/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object data_type enum (str) no ['json', 'csv'] will convert data to a dict based on type; requires get_object=True get_object bool no False will download object from s3 bucket and hold in memory operations list no ['created', 'updated', 'deleted'] will only run if record was created from the listed operation raise_operation_error bool no False will raise exception if operation of record is not from listed operations raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from acai_aws.s3.requirements import requirements # example data class (requires, get_object=True and a data_type) class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , get_object = True , data_type = 'json' , data_class = SomeClass , raise_body_error = True , # requires, get_object=True and a data_type schema = 'service/openapi.yml' , required_body = 'v1-s3-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record )","title":"Requirements Configuration Options"},{"location":"s3/configuration-details/#s3-record-properties","text":"property type description name str the name of the event source str the source of the event version str the version of the event time str the time of the event region str the region of the event request dict the request parameters of the event response dict the response parameters of the event configuration_id str the configuration id object dict the object dict of the event bucket dict the bucket dict of the event bucket dict the bucket dict of the event bucket_arn str the arn of the bucket bucket_owner str the owner of the bucket key str the key owner of the object schema_version str the schema version user_identity str the user identity operation enum (str) enum of created , deleted , updated body dict,bytes the body of file in the s3 bucket","title":"S3 Record Properties"},{"location":"s3/configuration-details/#recordname","text":"1 2 3 4 print ( record . name ) # output 'ObjectRemoved: Put'","title":"record.name"},{"location":"s3/configuration-details/#recordsource","text":"1 2 3 4 print ( record . source ) # output 'aws:s3'","title":"record.source"},{"location":"s3/configuration-details/#recordversion","text":"1 2 3 4 print ( record . version ) # output '2.0'","title":"record.version"},{"location":"s3/configuration-details/#recordtime","text":"1 2 3 4 print ( record . time ) # output '2018-09-20T21: 10: 13.821Z'","title":"record.time"},{"location":"s3/configuration-details/#recordregion","text":"1 2 3 4 print ( record . region ) # output 'us-east-1'","title":"record.region"},{"location":"s3/configuration-details/#recordrequest","text":"1 2 3 4 5 6 print ( record . request ) # output { 'sourceIPAddress' : '172.20.133.36' }","title":"record.request"},{"location":"s3/configuration-details/#recordresponse","text":"1 2 3 4 5 6 7 print ( record . response ) # output { 'x-amz-request-id' : '6B859DD0CE613FAE' , 'x-amz-id-2' : 'EXLMfc9aiXZFzNwLKXpw35iaVvl/DkEA6GtbuxjfmuLN3kLPL/aGoa7NMSwpl3m7ICAtNbjJX4w=' }","title":"record.response"},{"location":"s3/configuration-details/#recordconfiguration_id","text":"1 2 3 4 5 6 7 print ( record . configuration_id ) # output { 'x-amz-request-id' : '6B859DD0CE613FAE' , 'x-amz-id-2' : 'EXLMfc9aiXZFzNwLKXpw35iaVvl/DkEA6GtbuxjfmuLN3kLPL/aGoa7NMSwpl3m7ICAtNbjJX4w=' }","title":"record.configuration_id"},{"location":"s3/configuration-details/#recordobject","text":"1 2 3 4 5 6 7 8 9 print ( record . object ) # output { 'key' : 'user-1-prefs.json' , 'size' : 17545 , 'eTag' : 'b79ac2ef68c08fa9ac6013d53038a26c' , 'sequencer' : '005BA40CB5BD42013A' }","title":"record.object"},{"location":"s3/configuration-details/#recordbucket","text":"1 2 3 4 5 6 7 8 9 10 print ( record . bucket ) # output { 'name' : 'user-preferences' , 'ownerIdentity' : { 'principalId' : 'A32KFL0DQ3MH8X' }, 'arn' : 'arn:aws:s3:::user-preferences' }","title":"record.bucket"},{"location":"s3/configuration-details/#recordbucket_arn","text":"1 2 3 4 print ( record . bucket_arn ) # output 'arn:aws:s3:::user-preferences'","title":"record.bucket_arn"},{"location":"s3/configuration-details/#recordbucket_owner","text":"1 2 3 4 print ( record . bucket_owner ) # output 'A32KFL0DQ3MH8X'","title":"record.bucket_owner"},{"location":"s3/configuration-details/#recordkey","text":"1 2 3 4 print ( record . key ) # output 'user-1-prefs.json'","title":"record.key"},{"location":"s3/configuration-details/#recordschema_version","text":"1 2 3 4 print ( record . schema_version ) # output '1.0'","title":"record.schema_version"},{"location":"s3/configuration-details/#recorduser_identity","text":"1 2 3 4 print ( record . user_identity ) # output 'AWS: AROAI7Z5ZQEQ3UETKKYGQ: deploy-workers-poc-put-v1-photo'","title":"record.user_identity"},{"location":"s3/configuration-details/#recordoperation","text":"1 2 3 4 print ( record . operation ) # output 'created'","title":"record.operation"},{"location":"s3/configuration-details/#recordbody","text":"1 2 3 4 5 6 print ( record . body ) # output { 'possible' : 'output' }","title":"record.body"},{"location":"s3/quickstart/","text":"S3 Quickstart \u00b6 Event handler for Amazon S3 Bucket Events. Features \u00b6 Automatically convert JSON files to JSON objects Automatically convert CSV files to JSON objects Only run on certain S3 operations, like on when items are created, updated or deleted Able to validate S3 record against a JSON Schema Assign Data Classes to records instead of getting raw S3 JSON objects Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. s3.py serverless.yml 1 2 3 4 5 6 7 8 9 from acai_aws.s3.requirements import requirements @requirements ( get_object = True , data_type = 'json' ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 functions : s3-handler : handler : service/handlers/s3.handle memorySize : 512 timeout : 900 events : - s3 : bucket : uploadsBucket event : s3:ObjectCreated:*","title":"Quickstart"},{"location":"s3/quickstart/#s3-quickstart","text":"Event handler for Amazon S3 Bucket Events.","title":"S3 Quickstart"},{"location":"s3/quickstart/#features","text":"Automatically convert JSON files to JSON objects Automatically convert CSV files to JSON objects Only run on certain S3 operations, like on when items are created, updated or deleted Able to validate S3 record against a JSON Schema Assign Data Classes to records instead of getting raw S3 JSON objects","title":"Features"},{"location":"s3/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"s3/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. s3.py serverless.yml 1 2 3 4 5 6 7 8 9 from acai_aws.s3.requirements import requirements @requirements ( get_object = True , data_type = 'json' ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 functions : s3-handler : handler : service/handlers/s3.handle memorySize : 512 timeout : 900 events : - s3 : bucket : uploadsBucket event : s3:ObjectCreated:*","title":"Minimal Setup"},{"location":"sns/configuration-details/","text":"SNS Event Configurations \u00b6 The SNS event will automatically handle many common things done when eventing off an SNS stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 6 7 8 9 functions : sns-handler : handler : service/handlers/sns.handle memorySize : 512 timeout : 900 events : - sns : arn : Fn::GetAtt : [ SomeTopic , 'Arn' ] Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from acai_aws.sns.requirements import requirements # example data class (requires, get_object=True and a data_type) class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , data_class = SomeClass , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-sns-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record ) SNS Record Properties \u00b6 property type description source str the source of the event source_arn str the source arn of the event region str the region of the event body dict,str the body of file in the s3 bucket attributes dict the attributes dict of the message attributes; easier than use message attributes message_attributes dict the message attributes of the message version str the version sns event subscription_arn str the subscription arn of the sns event signature_version str the signature version of the sns event timestamp str the timestamp of the sns event signature str the signature of the sns event signing_cert_url str the signing cert url of the sns event message_id str the message id of the sns event message str the message of the sns event sns_type str the type of the sns event unsubscribe_url str the unsubscribe_url of the sns event topic_arn str the topic arn of the sns event subject str the subject of the sns event record.source \u00b6 1 2 3 4 print ( record . source ) # output 'aws:sns' record.source_arn \u00b6 1 2 3 4 print ( record . source_arn ) # output 'arn:aws:sns:us-east-2:123456789012:my-queue' record.region \u00b6 1 2 3 4 print ( record . region ) # output 'us-east-1' record.body \u00b6 1 2 3 4 5 6 print ( record . body ) # output { 'key' : 'value' } record.attributes \u00b6 1 2 3 4 5 6 7 print ( record . attributes ) # output { 'SomeString' : 'Some String' , 'SomeBinary' : 'Some Binary' , } record.message_attributes \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 print ( record . message_attributes ) # output { 'SomeString' : { 'DataType' : 'string' , 'StringValue' : 'Some String' }, 'SomeBinary' : { 'DataType' : 'binary' , 'BinaryValue' : 'Some Binary' } } record.version \u00b6 1 2 3 4 print ( record . version ) # output '1.0' record.subscription_arn \u00b6 1 2 3 4 print ( record . subscription_arn ) # output 'arn:aws:sns:us-east-1:123456789012:sns-lambda:21be56ed-a058-49f5-8c98-aedd2564c486' record.signature_version \u00b6 1 2 3 4 print ( record . signature_version ) # output '1' record.timestamp \u00b6 1 2 3 4 print ( record . timestamp ) # output '2019-01-02T12:45:07.000Z' record.signature \u00b6 1 2 3 4 print ( record . signature ) # output 'tcc6faL2yUC6dgZdmrwh1Y4cGa/ebXEkAi6RibDsvpi+tE/1+82j...65r==' record.signing_cert_url \u00b6 1 2 3 4 print ( record . signing_cert_url ) # output 'https://sns.us-east-1.amazonaws.com/SimpleNotificationService-ac565b8b1a6c5d002d285f9598aa1d9b.pem' record.signing_cert_url \u00b6 1 2 3 4 print ( record . signing_cert_url ) # output 'https://sns.us-east-1.amazonaws.com/SimpleNotificationService-ac565b8b1a6c5d002d285f9598aa1d9b.pem' record.message_id \u00b6 1 2 3 4 print ( record . message_id ) # output '95df01b4-ee98-5cb9-9903-4c221d41eb5e' record.message \u00b6 1 2 3 4 5 6 print ( record . message ) # output { 'key' : 'value' } record.sns_type \u00b6 1 2 3 4 print ( record . sns_type ) # output 'Notification' record.unsubscribe_url \u00b6 1 2 3 4 print ( record . unsubscribe_url ) # output 'https://sns.us-east-1.amazonaws.com/?Action=Unsubscribe&amp;SubscriptionArn=arn:aws:sns:us-east-1:123456789012:test-lambda:21be56ed-a058-49f5-8c98-aedd2564c486' record.topic_arn \u00b6 1 2 3 4 print ( record . topic_arn ) # output 'arn:aws:sns:us-east-1:123456789012:sns-lambda' record.subject \u00b6 1 2 3 4 print ( record . subject ) # output 'TestInvoke'","title":"Configurations"},{"location":"sns/configuration-details/#sns-event-configurations","text":"The SNS event will automatically handle many common things done when eventing off an SNS stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"SNS Event Configurations"},{"location":"sns/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 6 7 8 9 functions : sns-handler : handler : service/handlers/sns.handle memorySize : 512 timeout : 900 events : - sns : arn : Fn::GetAtt : [ SomeTopic , 'Arn' ]","title":"Lambda Configuration"},{"location":"sns/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from acai_aws.sns.requirements import requirements # example data class (requires, get_object=True and a data_type) class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , data_class = SomeClass , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-sns-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record )","title":"Requirements Configuration Options"},{"location":"sns/configuration-details/#sns-record-properties","text":"property type description source str the source of the event source_arn str the source arn of the event region str the region of the event body dict,str the body of file in the s3 bucket attributes dict the attributes dict of the message attributes; easier than use message attributes message_attributes dict the message attributes of the message version str the version sns event subscription_arn str the subscription arn of the sns event signature_version str the signature version of the sns event timestamp str the timestamp of the sns event signature str the signature of the sns event signing_cert_url str the signing cert url of the sns event message_id str the message id of the sns event message str the message of the sns event sns_type str the type of the sns event unsubscribe_url str the unsubscribe_url of the sns event topic_arn str the topic arn of the sns event subject str the subject of the sns event","title":"SNS Record Properties"},{"location":"sns/configuration-details/#recordsource","text":"1 2 3 4 print ( record . source ) # output 'aws:sns'","title":"record.source"},{"location":"sns/configuration-details/#recordsource_arn","text":"1 2 3 4 print ( record . source_arn ) # output 'arn:aws:sns:us-east-2:123456789012:my-queue'","title":"record.source_arn"},{"location":"sns/configuration-details/#recordregion","text":"1 2 3 4 print ( record . region ) # output 'us-east-1'","title":"record.region"},{"location":"sns/configuration-details/#recordbody","text":"1 2 3 4 5 6 print ( record . body ) # output { 'key' : 'value' }","title":"record.body"},{"location":"sns/configuration-details/#recordattributes","text":"1 2 3 4 5 6 7 print ( record . attributes ) # output { 'SomeString' : 'Some String' , 'SomeBinary' : 'Some Binary' , }","title":"record.attributes"},{"location":"sns/configuration-details/#recordmessage_attributes","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 print ( record . message_attributes ) # output { 'SomeString' : { 'DataType' : 'string' , 'StringValue' : 'Some String' }, 'SomeBinary' : { 'DataType' : 'binary' , 'BinaryValue' : 'Some Binary' } }","title":"record.message_attributes"},{"location":"sns/configuration-details/#recordversion","text":"1 2 3 4 print ( record . version ) # output '1.0'","title":"record.version"},{"location":"sns/configuration-details/#recordsubscription_arn","text":"1 2 3 4 print ( record . subscription_arn ) # output 'arn:aws:sns:us-east-1:123456789012:sns-lambda:21be56ed-a058-49f5-8c98-aedd2564c486'","title":"record.subscription_arn"},{"location":"sns/configuration-details/#recordsignature_version","text":"1 2 3 4 print ( record . signature_version ) # output '1'","title":"record.signature_version"},{"location":"sns/configuration-details/#recordtimestamp","text":"1 2 3 4 print ( record . timestamp ) # output '2019-01-02T12:45:07.000Z'","title":"record.timestamp"},{"location":"sns/configuration-details/#recordsignature","text":"1 2 3 4 print ( record . signature ) # output 'tcc6faL2yUC6dgZdmrwh1Y4cGa/ebXEkAi6RibDsvpi+tE/1+82j...65r=='","title":"record.signature"},{"location":"sns/configuration-details/#recordsigning_cert_url","text":"1 2 3 4 print ( record . signing_cert_url ) # output 'https://sns.us-east-1.amazonaws.com/SimpleNotificationService-ac565b8b1a6c5d002d285f9598aa1d9b.pem'","title":"record.signing_cert_url"},{"location":"sns/configuration-details/#recordsigning_cert_url_1","text":"1 2 3 4 print ( record . signing_cert_url ) # output 'https://sns.us-east-1.amazonaws.com/SimpleNotificationService-ac565b8b1a6c5d002d285f9598aa1d9b.pem'","title":"record.signing_cert_url"},{"location":"sns/configuration-details/#recordmessage_id","text":"1 2 3 4 print ( record . message_id ) # output '95df01b4-ee98-5cb9-9903-4c221d41eb5e'","title":"record.message_id"},{"location":"sns/configuration-details/#recordmessage","text":"1 2 3 4 5 6 print ( record . message ) # output { 'key' : 'value' }","title":"record.message"},{"location":"sns/configuration-details/#recordsns_type","text":"1 2 3 4 print ( record . sns_type ) # output 'Notification'","title":"record.sns_type"},{"location":"sns/configuration-details/#recordunsubscribe_url","text":"1 2 3 4 print ( record . unsubscribe_url ) # output 'https://sns.us-east-1.amazonaws.com/?Action=Unsubscribe&amp;SubscriptionArn=arn:aws:sns:us-east-1:123456789012:test-lambda:21be56ed-a058-49f5-8c98-aedd2564c486'","title":"record.unsubscribe_url"},{"location":"sns/configuration-details/#recordtopic_arn","text":"1 2 3 4 print ( record . topic_arn ) # output 'arn:aws:sns:us-east-1:123456789012:sns-lambda'","title":"record.topic_arn"},{"location":"sns/configuration-details/#recordsubject","text":"1 2 3 4 print ( record . subject ) # output 'TestInvoke'","title":"record.subject"},{"location":"sns/quickstart/","text":"SNS Quickstart \u00b6 Event handler for Amazon SQS Events. Features \u00b6 Automatically convert JSON from message body Automatically flatten message attributes Able to message body against a JSON Schema Assign Data Classes to records instead of getting record objects Installation \u00b6 Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. sns.py serverless.yml 1 2 3 4 5 6 7 8 from acai_aws.sns.requirements import requirements @requirements ( required_body = 'v1-sns-event' ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 functions : sns-handler : handler : service/handlers/sns.handle memorySize : 512 timeout : 900 events : - sqs : arn : Fn::GetAtt : [ SomeTopic , 'Arn' ]","title":"Quickstart"},{"location":"sns/quickstart/#sns-quickstart","text":"Event handler for Amazon SQS Events.","title":"SNS Quickstart"},{"location":"sns/quickstart/#features","text":"Automatically convert JSON from message body Automatically flatten message attributes Able to message body against a JSON Schema Assign Data Classes to records instead of getting record objects","title":"Features"},{"location":"sns/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai_aws # pipenv install acai_aws # poetry add acai_aws","title":"Installation"},{"location":"sns/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. sns.py serverless.yml 1 2 3 4 5 6 7 8 from acai_aws.sns.requirements import requirements @requirements ( required_body = 'v1-sns-event' ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 functions : sns-handler : handler : service/handlers/sns.handle memorySize : 512 timeout : 900 events : - sqs : arn : Fn::GetAtt : [ SomeTopic , 'Arn' ]","title":"Minimal Setup"},{"location":"sqs/configuration-details/","text":"SQS Event Configurations \u00b6 The SQS event will automatically handle many common things done when eventing off an SQS stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account! Lambda Configuration \u00b6 serverless.yml 1 2 3 4 5 6 7 8 9 functions : sqs-handler : handler : service/handlers/sqs.handle memorySize : 512 timeout : 900 events : - sqs : arn : Fn::GetAtt : [ SomeQueue , 'Arn' ] Requirements Configuration Options \u00b6 option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from acai.sqs.requirements import requirements # example data class (requires, get_object=True and a data_type) class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , data_class = SomeClass , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-sqs-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record ) SQS Record Properties \u00b6 property type description source str the source of the event source_arn str the source arn of the event region str the region of the event body dict,str the body sqs message md5_of_body str the md5 of the body attributes dict the attributes dict of the message attributes; easier than use message attributes message_attributes dict the message attributes of the message record.source \u00b6 1 2 3 4 print ( record . source ) # output 'aws:sqs' record.source_arn \u00b6 1 2 3 4 print ( record . source_arn ) # output 'arn:aws:sqs:us-east-2:123456789012:my-queue' record.region \u00b6 1 2 3 4 print ( record . region ) # output 'us-east-1' record.body \u00b6 1 2 3 4 5 6 print ( record . body ) # output { 'key' : 'value' } record.md5_of_body \u00b6 1 2 3 4 print ( record . md5_of_body ) # output 'e4e68fb7bd0e697a0ae8f1bb342846b3' record.attributes \u00b6 1 2 3 4 5 6 7 print ( record . attributes ) # output { 'SomeString' : 'Some String' , 'SomeBinary' : 'Some Binary' , } record.message_attributes \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 print ( record . message_attributes ) # output { 'SomeString' : { 'DataType' : 'string' , 'StringValue' : 'Some String' }, 'SomeBinary' : { 'DataType' : 'binary' , 'BinaryValue' : 'Some Binary' } }","title":"Configurations"},{"location":"sqs/configuration-details/#sqs-event-configurations","text":"The SQS event will automatically handle many common things done when eventing off an SQS stream. Developers then have the ability to further extend that functionality with custom middleware. Below is a full list of all the configurations available and examples of their use. Examples Don't like reading documentation? Then look at our examples, which can be deployed in 1 command into your AWS account!","title":"SQS Event Configurations"},{"location":"sqs/configuration-details/#lambda-configuration","text":"serverless.yml 1 2 3 4 5 6 7 8 9 functions : sqs-handler : handler : service/handlers/sqs.handle memorySize : 512 timeout : 900 events : - sqs : arn : Fn::GetAtt : [ SomeQueue , 'Arn' ]","title":"Lambda Configuration"},{"location":"sqs/configuration-details/#requirements-configuration-options","text":"option type required default description before func no None a custom function to be ran before your records are pulled after func no None a custom function to be ran after your records are pulled data_class class no None a custom class that will be passed instead of the records object raise_body_error bool no False will raise exception if body of record does not match schema provided required_body str or dict no None will validate body of record against this schema schema str no None file path pointing to the location of the openapi.yml file 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from acai.sqs.requirements import requirements # example data class (requires, get_object=True and a data_type) class SomeClass : def __init__ ( self , record ): for k , v in record . body . items (): setattr ( self , k , v ) # example before function def log_something ( records , requirements ): if 'something' in requirements : print ( records ) # example after function def alert_something ( records , result , requirements ): if 'something' in result and 'alert' in requirements : print ( records ) @requirements ( before = log_something , data_class = SomeClass , raise_body_error = True , schema = 'service/openapi.yml' , required_body = 'v1-sqs-body' , # or send jsonschema dict; schema kwarg not needed if sending jsonschema dict after = alert_something , ) def handle ( event ): for record in event . records : print ( record )","title":"Requirements Configuration Options"},{"location":"sqs/configuration-details/#sqs-record-properties","text":"property type description source str the source of the event source_arn str the source arn of the event region str the region of the event body dict,str the body sqs message md5_of_body str the md5 of the body attributes dict the attributes dict of the message attributes; easier than use message attributes message_attributes dict the message attributes of the message","title":"SQS Record Properties"},{"location":"sqs/configuration-details/#recordsource","text":"1 2 3 4 print ( record . source ) # output 'aws:sqs'","title":"record.source"},{"location":"sqs/configuration-details/#recordsource_arn","text":"1 2 3 4 print ( record . source_arn ) # output 'arn:aws:sqs:us-east-2:123456789012:my-queue'","title":"record.source_arn"},{"location":"sqs/configuration-details/#recordregion","text":"1 2 3 4 print ( record . region ) # output 'us-east-1'","title":"record.region"},{"location":"sqs/configuration-details/#recordbody","text":"1 2 3 4 5 6 print ( record . body ) # output { 'key' : 'value' }","title":"record.body"},{"location":"sqs/configuration-details/#recordmd5_of_body","text":"1 2 3 4 print ( record . md5_of_body ) # output 'e4e68fb7bd0e697a0ae8f1bb342846b3'","title":"record.md5_of_body"},{"location":"sqs/configuration-details/#recordattributes","text":"1 2 3 4 5 6 7 print ( record . attributes ) # output { 'SomeString' : 'Some String' , 'SomeBinary' : 'Some Binary' , }","title":"record.attributes"},{"location":"sqs/configuration-details/#recordmessage_attributes","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 print ( record . message_attributes ) # output { 'SomeString' : { 'DataType' : 'string' , 'StringValue' : 'Some String' }, 'SomeBinary' : { 'DataType' : 'binary' , 'BinaryValue' : 'Some Binary' } }","title":"record.message_attributes"},{"location":"sqs/quickstart/","text":"SQS Quickstart \u00b6 Event handler for Amazon SQS Events. Features \u00b6 Automatically convert JSON from message body Automatically flatten message attributes Able to message body against a JSON Schema Assign Data Classes to records instead of getting record objects Installation \u00b6 Shell 1 2 3 $ pip install acai # pipenv install acai # poetry add acai Minimal Setup \u00b6 After installation, create a handler file and configure the AWS lambda to use that file as its handler. sqs.py serverless.yml 1 2 3 4 5 6 7 8 from acai_aws.sqs.requirements import requirements @requirements ( required_body = 'v1-sqs-event' ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 functions : sqs-handler : handler : service/handlers/sqs.handle memorySize : 512 timeout : 900 events : - sqs : arn : Fn::GetAtt : [ SomeQueue , 'Arn' ]","title":"Quickstart"},{"location":"sqs/quickstart/#sqs-quickstart","text":"Event handler for Amazon SQS Events.","title":"SQS Quickstart"},{"location":"sqs/quickstart/#features","text":"Automatically convert JSON from message body Automatically flatten message attributes Able to message body against a JSON Schema Assign Data Classes to records instead of getting record objects","title":"Features"},{"location":"sqs/quickstart/#installation","text":"Shell 1 2 3 $ pip install acai # pipenv install acai # poetry add acai","title":"Installation"},{"location":"sqs/quickstart/#minimal-setup","text":"After installation, create a handler file and configure the AWS lambda to use that file as its handler. sqs.py serverless.yml 1 2 3 4 5 6 7 8 from acai_aws.sqs.requirements import requirements @requirements ( required_body = 'v1-sqs-event' ) def handle ( event ): for record in event . records : print ( record ) 1 2 3 4 5 6 7 8 9 functions : sqs-handler : handler : service/handlers/sqs.handle memorySize : 512 timeout : 900 events : - sqs : arn : Fn::GetAtt : [ SomeQueue , 'Arn' ]","title":"Minimal Setup"}]}